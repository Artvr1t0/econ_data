{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bd econ CPS extract\n",
    "\n",
    "bd_CPS_revisions_reader.ipynb\n",
    "\n",
    "March 6, 2019\n",
    "\n",
    "Contact: Brian Dew, @bd_econ\n",
    "\n",
    "-----\n",
    "\n",
    "Reads in Census revised data and stores it as feather files for merging with the bd CPS. This also separately incldues revised union data.\n",
    "\n",
    "**2000-based revised weights:**\n",
    "\n",
    "Requires: `2000-2extract.txt` data dictionary and unzipped 2000 Based Public Use Extracts from [Census](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract). \n",
    "\n",
    "**2001-2002 revised union data:**\n",
    "\n",
    "Requires: `2000-2extract.txt` data dictionary and unzipped 2000 Based Public Use Extracts from [Census](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract). \n",
    "\n",
    "**2000-2002 recoded occupation and industry:**\n",
    "\n",
    "Requires: `2000-2extract.txt` data dictionary and unzipped 2000 Based Public Use Extracts from [Census](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract). \n",
    "\n",
    "**December 2007 revised weights:**\n",
    "\n",
    "Requires: `dec07revwgts_dd.txt` data dictionary and unzipped `dec07revwgts.dat` from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html). \n",
    "\n",
    "-----\n",
    "\n",
    "Reads in Certification extracts for 2015 and 2016, which identify people with a professional certification, and go into the bd CPS variable CERT.\n",
    "\n",
    "**2015-2016 certification data:**\n",
    "\n",
    "Requires `Certification_extract_file_YYYY_rec_layout.txt` data dictionaries for YYYY in 2015 and 2016, and unzipped data files from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpscert).\n",
    "\n",
    "-----\n",
    "\n",
    "Reads disability flag for late 2008 observations.\n",
    "\n",
    "**June to December 2008 disability flag data:**\n",
    "\n",
    "Requires data dictionaries for YYYY in 2015 and 2016, and unzipped data files from [Census CPS FTP](https://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic_extract).\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "See [related GitHub issue](https://github.com/bdecon/econ_data/issues/82) for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T16:18:28.359096Z",
     "start_time": "2020-09-02T16:18:28.203268Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 1.1.1\n",
      "numpy: 1.19.1\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries (python 3.7)\n",
    "import os, re, struct\n",
    "import pandas as pd\n",
    "print('pandas:', pd.__version__)\n",
    "import numpy as np\n",
    "print('numpy:', np.__version__)\n",
    "\n",
    "os.chdir('/home/brian/Documents/CPS/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T16:18:30.995833Z",
     "start_time": "2020-09-02T16:18:30.988025Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# User-defined functions\n",
    "def id_dtype(size):\n",
    "    '''Return data type based on fixed-width size'''\n",
    "    size = int(size)\n",
    "    dtype = ('intp' if size > 9 \n",
    "             else 'int32' if size > 4 \n",
    "             else 'int16' if size > 2 \n",
    "             else 'int8')\n",
    "    return dtype\n",
    "\n",
    "def data_dict_reader(dd_file, var_names):\n",
    "    '''Read data dictionary and return variable locations'''\n",
    "#    if dd_file == 'Disability_extract_file_2008_rec_layout_revised.txt':\n",
    "#        data_dict = open(dd_file, 'r', encoding='iso-8859-1').read()\n",
    "#        p = f'\\n(?:\\x0c)?({\"|\".join(var_names)})\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)'\n",
    "    data_dict = open(dd_file, 'r', encoding='iso-8859-1').read()\n",
    "    p = f'({\"|\".join(var_names)})\\s+(\\d+)\\s+.*? \\s+.*?(\\d+).*?(\\d\\d*)'\n",
    "    d = {s[0]: [int(s[2])-1, int(s[3]), f'{s[1]}s', id_dtype(s[1])]\n",
    "         for s in re.findall(p, data_dict)}\n",
    "    return d\n",
    "\n",
    "def struct_unpacker(d):\n",
    "    '''Return struct unpacker from variable locations'''\n",
    "    start, end, width, size = zip(*d.values())\n",
    "    skip = ([f'{s - e}x' for s, e in zip(start, [0] + list(end[:-1]))])\n",
    "    unpack_fmt = ''.join([j for i in zip(skip, width) for j in i])\n",
    "    return struct.Struct(unpack_fmt).unpack_from\n",
    "\n",
    "def data_file_reader(file, unpacker, dtypes, wgt):\n",
    "    '''Convert raw monthly file to dataframe'''\n",
    "    raw_data = open(file, 'rb')\n",
    "    data = [unpacker(row) for row in raw_data]\n",
    "    np_data = np.array(data, dtype=dtypes)\n",
    "    df = pd.DataFrame(np_data[np_data[wgt] > 0])\n",
    "    return df\n",
    "\n",
    "def df_adjuster(df, wgt_vars=None):\n",
    "    '''Adjust dataframe to match with bd CPS'''\n",
    "    rev_df = (df.rename({'HRYEAR4': 'YEAR', 'HRMONTH': 'MONTH'}, axis=1)\n",
    "                .assign(YEAR = lambda x: pd.Categorical(x['YEAR'])))\n",
    "    if wgt_vars != None:\n",
    "        rev_df[wgt_vars] = rev_df[wgt_vars] / 10000\n",
    "    return rev_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T06:11:49.548805Z",
     "start_time": "2020-09-02T06:11:49.546902Z"
    }
   },
   "source": [
    "#### Map person weight to HH weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T16:20:10.170760Z",
     "start_time": "2020-09-02T16:19:57.725652Z"
    }
   },
   "outputs": [],
   "source": [
    "var_names = ['HWHHWGT', 'QSTNUM', 'PWSSWGT', 'OCCURNUM']\n",
    "p = 'D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+'\n",
    "data_dict = open('jan98dd.asc', 'r', encoding='iso-8859-1').read()\n",
    "d = {s[0]: [int(s[2])-1, int(s[2])+int(s[1])-1, f'{s[1]}s'] \n",
    "     for s in re.findall(p, data_dict) if s[0] in var_names} \n",
    "start, end, width = zip(*d.values())\n",
    "skip = ([f'{s - e}x' for s, e in zip(start, [0] + list(end[:-1]))])\n",
    "unpack_fmt = ''.join([j for i in zip(skip, width) for j in i])\n",
    "unpacker = struct.Struct(unpack_fmt).unpack_from\n",
    "\n",
    "hhw = {2000: {}, 2001: {}, 2002: {}}\n",
    "for date in pd.date_range(start='2000-01-01', end='2002-12-01', freq='MS'):\n",
    "    file = f'{date.strftime(\"%b%y\").lower()}pub.dat'\n",
    "    raw_data = open(file, 'rb').readlines()\n",
    "    wgt = d['PWSSWGT']  \n",
    "    data = [[*map(int, unpacker(row))] for row in raw_data\n",
    "            if int(row[wgt[0]:wgt[1]]) > 0]\n",
    "    df = pd.DataFrame(data, columns=d.keys())\n",
    "    res = (df.query('HWHHWGT == PWSSWGT')\n",
    "             .drop_duplicates('QSTNUM')\n",
    "             [['QSTNUM', 'OCCURNUM']])\n",
    "    hhw[date.year][date.month] = res.set_index('QSTNUM')['OCCURNUM'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised 2000-based weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T16:20:37.977523Z",
     "start_time": "2020-09-02T16:20:29.363230Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store 2000-based revised weights as feather file\n",
    "dd_file = '2000-2extract.txt'\n",
    "\n",
    "var_names = ['HRMONTH', 'HRYEAR4', 'QSTNUM', 'OCCURNUM', \n",
    "             'NWCMPWGT', 'NWORWGT', 'NWSSWGT']\n",
    "\n",
    "wgt_vars = ['NWCMPWGT', 'NWORWGT', 'NWSSWGT']\n",
    "\n",
    "filter_wgt = 'NWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "rev_data_path = 'pubuse2000_2002/'\n",
    "\n",
    "file_names = os.listdir(rev_data_path)\n",
    "\n",
    "files = [rev_data_path + month_file for month_file in file_names]\n",
    "\n",
    "df = pd.concat([data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "                for file in files])\n",
    "\n",
    "for year in [2000, 2001, 2002]:\n",
    "    dfy = df.query('HRYEAR4 == @year')\n",
    "\n",
    "    dfy = df_adjuster(dfy, wgt_vars=wgt_vars)\n",
    "    \n",
    "    hhwy = (pd.DataFrame([(k, g, i) for k, v in hhw[year].items() for g, i in v.items()], \n",
    "                    columns=['MONTH', 'QSTNUM', 'HHWLN']))\n",
    "    dfy = dfy.merge(hhwy)\n",
    "    hhws = dfy.query('OCCURNUM == HHWLN')[['MONTH', 'QSTNUM', 'NWSSWGT']].rename({'NWSSWGT': 'NWHHWGT'}, axis=1)\n",
    "    dfy = dfy.merge(hhws).drop('HHWLN', axis=1)\n",
    "\n",
    "    dfy.to_feather(f'clean/cps_wgt_rev{year}.ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised union data (2001-2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T01:40:13.722732Z",
     "start_time": "2019-02-28T01:40:08.719133Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store revised union data as feather file\n",
    "dd_file = '2000-2extract.txt'\n",
    "\n",
    "var_names = ['HRMONTH', 'HRYEAR4', 'QSTNUM', 'OCCURNUM', \n",
    "             'NEERNLAB', 'NEERNCOV', 'NWSSWGT']\n",
    "\n",
    "filter_wgt = 'NWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "rev_data_path = 'pubuse2000_2002/'\n",
    "\n",
    "file_names = [file for file in os.listdir(rev_data_path) if file[3:5] != '00']\n",
    "\n",
    "files = [rev_data_path + month_file for month_file in file_names]\n",
    "\n",
    "df = pd.concat([data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "                for file in files])\n",
    "\n",
    "for year in [2001, 2002]:\n",
    "    dfy = df.query('HRYEAR4 == @year')\n",
    "\n",
    "    dfy = df_adjuster(dfy).drop('NWSSWGT', axis=1)\n",
    "\n",
    "    dfy.to_feather(f'clean/cps_union_rev{year}.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoded industry and occupation 2000-2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T04:00:25.824839Z",
     "start_time": "2019-03-07T04:00:10.185208Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Store 2000-based revised weights as feather file\n",
    "dd_file = '2000-2extract.txt'\n",
    "\n",
    "io_vars = ['NEIO1ICD', 'NEIO2ICD', 'NRDTIND1', 'NRDTIND2', 'NRDTOCC1', \n",
    "           'NRDTOCC2', 'NRMJIND1', 'NRMJIND2', 'NRMJOCC1', 'NRMJOCC2', \n",
    "           'NTIO1OCD', 'NTIO2OCD']\n",
    "\n",
    "var_names = ['HRMONTH', 'HRYEAR4', 'QSTNUM', 'OCCURNUM', 'NWSSWGT'] + io_vars\n",
    "\n",
    "filter_wgt = 'NWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "# Manually fixing -- bug to be resolved later\n",
    "d['NEIO1ICD'] = [13, 17, '4s', 'int16']\n",
    "\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "rev_data_path = 'pubuse2000_2002/'\n",
    "\n",
    "file_names = os.listdir(rev_data_path)\n",
    "\n",
    "files = [rev_data_path + month_file for month_file in file_names]\n",
    "\n",
    "df = pd.concat([data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "                for file in files])\n",
    "\n",
    "df = df.drop(['NWSSWGT'], axis=1)\n",
    "\n",
    "\n",
    "df = df.rename({'NEIO1ICD': 'IND', 'NEIO2ICD': 'IND2', 'NTIO1OCD': 'OCC',\n",
    "                'NRDTIND1': 'INDD', 'NRDTIND2': 'IND2D', 'NTIO2OCD': 'OCC2',\n",
    "                'NRDTOCC1': 'OCCD', 'NRDTOCC2': 'OCC2D', 'NRMJIND1': 'INDM',\n",
    "                'NRMJIND2': 'IND2M', 'NRMJOCC1': 'OCCM', 'NRMJOCC2': 'OCC2M'}, axis=1)\n",
    "\n",
    "\n",
    "for year in [2000, 2001, 2002]:\n",
    "    dfy = df.query('HRYEAR4 == @year')\n",
    "\n",
    "    dfy = df_adjuster(dfy)\n",
    "\n",
    "    dfy.to_feather(f'clean/cps_io_rev{year}.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised December 2007 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T01:40:13.992650Z",
     "start_time": "2019-02-28T01:40:13.724441Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store revised union data as feather file\n",
    "dd_file = 'dec07revwgts_dd.txt'\n",
    "\n",
    "var_names = ['QSTNUM', 'OCCURNUM', 'PWSSWGT', 'PWCMPWGT']\n",
    "\n",
    "filter_wgt = 'PWSSWGT'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "file = 'dec07revwgts.dat'\n",
    "\n",
    "# Special code to remove rows with only '.'\n",
    "with open(file, 'rb+') as f:\n",
    "    new_f = f.readlines()\n",
    "    f.seek(0)\n",
    "    for line in new_f:\n",
    "        if b'.' not in line:\n",
    "            f.write(line)\n",
    "    f.truncate()\n",
    "\n",
    "df = data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "\n",
    "df[['PWSSWGT', 'PWCMPWGT']] = df[['PWSSWGT', 'PWCMPWGT']] / 10000\n",
    "\n",
    "df = df.rename({'PWSSWGT': 'NWSSWGT', 'PWCMPWGT': 'NWCMPWGT'}, axis=1)\n",
    "\n",
    "df.reset_index(drop=True).to_feather('clean/cps_dec07_rev.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Certification data for 2015-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T01:40:16.137125Z",
     "start_time": "2019-02-28T01:40:13.993778Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store certification data as feather file\n",
    "\n",
    "# Use the 2016 dictionary for both years\n",
    "dd_file = 'Certification_extract_file_2016_rec_layout.txt'\n",
    "\n",
    "var_names = ['QSTNUM', 'PULINENO', 'MONTH', 'PECERT1']\n",
    "\n",
    "filter_wgt = 'PECERT1'\n",
    "\n",
    "d = data_dict_reader(dd_file, var_names)\n",
    "\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "# Loop over two years and create feather for each\n",
    "for year in ['2015', '2016']:\n",
    "    file = f'jan{year[2:]}-dec{year[2:]}cert_ext.dat'\n",
    "\n",
    "    df = data_file_reader(file, unpacker, dtypes, filter_wgt) \n",
    "\n",
    "    df.reset_index(drop=True).to_feather(f'clean/cps_cert{year}.ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disability Flag late 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T01:40:16.761327Z",
     "start_time": "2019-02-28T01:40:16.140849Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Store disability data as feather file\n",
    "\n",
    "d = {'QSTNUM': [0, 8, '8s', 'int32'],\n",
    "     'HRMONTH': [8, 16, '8s', 'int8'],\n",
    "     'OCCURNUM': [24, 32, '8s', 'int32'],\n",
    "     'PRDISFLG': [80, 88, '8s', 'int32']}\n",
    "dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "unpacker = struct_unpacker(d)\n",
    "\n",
    "# Unpack data\n",
    "file = 'disability.dat'\n",
    "\n",
    "df = data_file_reader(file, unpacker, dtypes, 'QSTNUM') \n",
    "\n",
    "df = df.rename({'HRMONTH': 'MONTH'}, axis=1)\n",
    "\n",
    "df.reset_index(drop=True).to_feather(f'clean/cps_disability2008.ft')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
