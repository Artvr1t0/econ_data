{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique CPS Household ID \n",
    "\n",
    "April 30, 2019\n",
    "\n",
    "Brian Dew, @bd_econ\n",
    "\n",
    "-----\n",
    "\n",
    "ABOUT:\n",
    "\n",
    "This file uses struct to read a monthly CPS file and then, based on dates and month in sample, merges the monthly file with the bd CPS feather file that should contain the same households. When the bd CPS file contains the same household, a new CPSID is generated based on the date and QSTNUM of the bd CPS file. When there is no match in the first possible month (where MIS=1 should be), the program continues to look. If it does not find a match, it generates a new CPSID based on the date and QSTNUM of the current month.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Drawn primarily from the description of the IPUMS CPSID. Works currently for May 1995-onward.\n",
    "\n",
    "**WORK IN PROGRESS**\n",
    "\n",
    "Eventually, this will have three sections. One that handles the feather files for 1989-94, a second that handles the dates right around and affected by the mid-1995 break, and a third that handles raw CPS files after the mid-1995 break.\n",
    "\n",
    "I could also try to create a format that handles feather files when they are available and raw files only when they are not already included in the feather file. \n",
    "\n",
    "The overall goal of this file should be: \n",
    "1) Check what months are covered by raw data files. \n",
    "2) Check whether CPSIDs are available for those months.\n",
    "3) If CPSIDs are missing, generate them and store them in the dictionary.\n",
    "\n",
    "In `bd_CPS_reader.ipynb` the CPSID should be generated only if the dictionary contains that month of data. To efficiently do this, I should read the dictionary once and generate a list of available months to cross check. That way, I can generate a bd CPS feather file without the ID, then use that feather file to generat the CPSIDs more efficiently. Once the CPSIDs are generated locally, this won't apply. Separately, I'll need to be able to generate a CPSID from a raw data file, so that I can add new months of data efficiently as they are released. \n",
    "\n",
    "Notes:\n",
    "\n",
    "- One issue when creating QSTNUM is that I use a different process to read the data in this notebook and so the QSTNUM generated here will not match the QSTNUM generated in the reader. Therefore, a dictionary maps the QSTNUM to HHIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:13:28.348003Z",
     "start_time": "2019-11-12T03:13:26.772561Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 1.0.3\n",
      "numpy: 1.18.1\n",
      "ID dictionary file exists\n"
     ]
    }
   ],
   "source": [
    "# Import preliminaries\n",
    "import os, re, struct, pickle, string, sys\n",
    "import pandas as pd\n",
    "print('pandas:', pd.__version__)\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "print('numpy:', np.__version__)\n",
    "from bd_CPS_details import StatesMap, DataDict\n",
    "\n",
    "os.chdir('/home/brian/Documents/CPS/data')\n",
    "\n",
    "#sys.stdout = open('cps_id_log.txt', 'w')\n",
    "\n",
    "dd_matcher = pickle.load(open('cps_basic_dd.pkl', 'rb'))['matcher']\n",
    "\n",
    "\n",
    "# Storage of IDs in pickled dictionary\n",
    "ids_file = 'CPS_unique_ids.pkl'\n",
    "if os.path.isfile(ids_file):\n",
    "    print('ID dictionary file exists')\n",
    "    cps_ids_full = pickle.load(open(ids_file, 'rb'))\n",
    "else:\n",
    "    cps_ids_full = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:13:28.356416Z",
     "start_time": "2019-11-12T03:13:28.349226Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Return regex pattern that will parse data dictionary dd_file\n",
    "def return_dd_parser(dd_file):\n",
    "    \n",
    "    DataDict = {'January_2020_Record_Layout.txt': \n",
    "                '\\n(\\w+)\\s+(\\d+)\\s+.*?\\t+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'January_2017_Record_Layout.txt': \n",
    "                '\\n(\\w+)\\s+(\\d+)\\s+.*?\\t+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'January_2015_Record_Layout.txt':\n",
    "                '\\n(\\w+)\\s+(\\d+)\\s+.*?\\t+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'January_2014_Record_Layout.txt':\n",
    "                '\\n(\\w+)\\s+(\\d+)\\s+.*?\\t+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'January_2013_Record_Layout.txt':\n",
    "                '\\n(\\w+)\\s+(\\d+)\\s+.*?\\t+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'may12dd.txt':\n",
    "                '\\n(\\w+)\\s+(\\d+)\\s+.*?\\t+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'jan10dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'jan09dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'jan07dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'augnov05dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'may04dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'jan03dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'jan98dd.asc':\n",
    "                'D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+',\n",
    "                'jan98dd2.asc':\n",
    "                'D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+',\n",
    "                'sep95_dec97_dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'jun95_aug95_dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'apr94_may95_dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)',\n",
    "                'jan94_mar94_dd.txt':\n",
    "                '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+.*? \\s+.*?(\\d\\d*).*?(\\d\\d+)'}\n",
    "    \n",
    "    return DataDict[dd_file]\n",
    "\n",
    "\n",
    "# Create HHID2 for pre May 2004 data\n",
    "def id2_gen(np_mo):\n",
    "    hrsample = [x[1:3] for x in np_mo['HRSAMPLE']]\n",
    "    hrsersuf = [x.strip() for x in np_mo['HRSERSUF']]\n",
    "    sersuf_d = {a: str(ord(a.lower()) - 96).zfill(2) for a in set(hrsersuf)\n",
    "            if a in list(string.ascii_letters)}\n",
    "    sersuf_d.update({'-1': '00', '-1.0': '00', '0': '00'})\n",
    "    sersuf = list(map(sersuf_d.get, hrsersuf))\n",
    "    np_mo.loc[np_mo['HUHHNUM'] < 0, 'HUHHNUM'] = 0\n",
    "    huhhnum = np_mo['HUHHNUM'].astype('U1')\n",
    "    \n",
    "    id2 = [''.join(i) for i in zip(hrsample, sersuf, huhhnum)]\n",
    "\n",
    "    return(np.array(id2, dtype='uint32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:13:28.457137Z",
     "start_time": "2019-11-12T03:13:28.357863Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# List of monthly raw CPS data files to process\n",
    "raw_monthly_data_file_list = [file for file in os.listdir() \n",
    "                              if file.endswith('pub.dat') \n",
    "                              and (pd.to_datetime(file[:5], format='%b%y')\n",
    "                                   >= pd.to_datetime('1995-05-01')) and\n",
    "                              (pd.to_datetime(file[:5], format='%b%y')\n",
    "                                   not in cps_ids_full.keys())]\n",
    "\n",
    "#raw_monthly_data_file_list = ['nov04pub.dat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:13:28.463537Z",
     "start_time": "2019-11-12T03:13:28.458205Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# For 1995-1997 map the HHID and HHID2 to QSTNUM\n",
    "pre98files = [f[3:5] for f in raw_monthly_data_file_list \n",
    "              if f[3:5] in ['95', '96', '97']]\n",
    "\n",
    "if len(pre98files) > 0:\n",
    "    qstnum_map_file = 'qstnum_map.pkl'\n",
    "    if os.path.isfile(qstnum_map_file):\n",
    "        print('QSTNUM mapping dictionary file exists')\n",
    "        qstnum_map = pickle.load(open(qstnum_map_file, 'rb'))\n",
    "    else:\n",
    "        qstnum_map = {}\n",
    "\n",
    "        columns = ['MONTH', 'HHID', 'HHID2', 'QSTNUM']\n",
    "\n",
    "        for year in [1995, 1996, 1997]:\n",
    "            df = pd.read_feather(f'clean/cps{year}.ft', columns=columns)\n",
    "            df['ID'] = df['HHID'].astype('str') + df['HHID2'].astype('str')\n",
    "            for month in df['MONTH'].unique():\n",
    "                date = (year * 100 + month) % 10000\n",
    "                dfm = df[df['MONTH'] == month].copy()\n",
    "                qstnum_map[date] = dfm.set_index('ID')['QSTNUM'].to_dict()\n",
    "        # Write to file\n",
    "        with open(qstnum_map_file, 'wb') as f:\n",
    "            pickle.dump(qstnum_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:13:30.256855Z",
     "start_time": "2019-11-12T03:13:28.464596Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current month: 2020-05-01 00:00:00\n",
      "Total HHs in sample: 40743\n",
      "New HHs (MIS1): 3705\n",
      "Matched HHs (MIS2):  3246\n",
      "Replacement HHs (MIS2):  993\n",
      "Matched HHs (MIS3):  3727\n",
      "Matched HHs (MIS3):  886\n",
      "Replacement HHs (MIS3):  475\n",
      "Matched HHs (MIS4):  5045\n",
      "Matched HHs (MIS4):  288\n",
      "Matched HHs (MIS4):  211\n",
      "Replacement HHs (MIS4):  139\n",
      "Matched HHs (MIS5):  4159\n",
      "Matched HHs (MIS5):  374\n",
      "Matched HHs (MIS5):  145\n",
      "Matched HHs (MIS5):  114\n",
      "Replacement HHs (MIS5):  329\n",
      "Matched HHs (MIS6):  4238\n",
      "Matched HHs (MIS6):  345\n",
      "Matched HHs (MIS6):  145\n",
      "Matched HHs (MIS6):  90\n",
      "Matched HHs (MIS6):  286\n",
      "Replacement HHs (MIS6):  164\n",
      "Matched HHs (MIS7):  4371\n",
      "Matched HHs (MIS7):  422\n",
      "Matched HHs (MIS7):  164\n",
      "Matched HHs (MIS7):  103\n",
      "Matched HHs (MIS7):  349\n",
      "Matched HHs (MIS7):  139\n",
      "Replacement HHs (MIS7):  142\n",
      "Matched HHs (MIS8):  4550\n",
      "Matched HHs (MIS8):  368\n",
      "Matched HHs (MIS8):  140\n",
      "Matched HHs (MIS8):  92\n",
      "Matched HHs (MIS8):  561\n",
      "Matched HHs (MIS8):  86\n",
      "Matched HHs (MIS8):  74\n",
      "Replacement HHs (MIS8):  78\n",
      "Total IDs created: 40743\n",
      "Total IDs not created: 0 \n",
      "\n",
      "\n",
      "Total months of IDs: 301\n"
     ]
    }
   ],
   "source": [
    "# Loop over files of interest and generate unique IDs\n",
    "for file in raw_monthly_data_file_list:\n",
    "    # Details for matching new file to previous data\n",
    "    curr_mo = pd.to_datetime(file[:5], format='%b%y')\n",
    "    curr_mo_short = int(curr_mo.strftime('%y%m'))\n",
    "    \n",
    "    # Handling dates before and at break\n",
    "    if curr_mo < pd.to_datetime('1995-05-01'):\n",
    "        continue\n",
    "\n",
    "    print('Current month:', curr_mo)\n",
    "    \n",
    "    # Identify possible matching months\n",
    "    mo_diffs = [1, 2, 3, 9, 10, 11, 12, 13, 14, 15]\n",
    "    poss_mos = [poss_mo for poss_mo in [curr_mo - pd.DateOffset(months=mo_diff)\n",
    "                for mo_diff in mo_diffs] \n",
    "                if poss_mo >= pd.to_datetime('1995-05-01')]\n",
    "    \n",
    "    # Put in format to match with bd CPS data\n",
    "    yymms = [int(pm.strftime('%y%m')) for pm in poss_mos]\n",
    "\n",
    "    # Which annual bd CPS files to pull\n",
    "    years = list(set([pm.year for pm in poss_mos]))\n",
    "    if curr_mo == pd.to_datetime('1995-05-01'):\n",
    "        yymms = ['9505']\n",
    "        years = [1995]\n",
    "    bd_CPS_files = [f'cps{year}.ft' for year in years]\n",
    "    \n",
    "    # For each month in sample, which months can match?\n",
    "    match_months = {\n",
    "        2: [1],\n",
    "        3: [2, 1],\n",
    "        4: [3, 2, 1],\n",
    "        5: [12, 11, 10, 9],\n",
    "        6: [13, 12, 11, 10, 1],\n",
    "        7: [14, 13, 12, 11, 2, 1],\n",
    "        8: [15, 14, 13, 12, 3, 2, 1]\n",
    "    }\n",
    "    \n",
    "    # Return list of yymms to search for each MIS based on curr_mo\n",
    "    search_list = {mis: [int(search_mo.strftime('%y%m')) for search_mo in \n",
    "                         [curr_mo - pd.DateOffset(months=mo_diff) \n",
    "                          for mo_diff in match_months[mis]] \n",
    "                         if search_mo > pd.to_datetime('1995-08-01')]\n",
    "                   for mis in [2, 3, 4, 5, 6, 7, 8]}\n",
    "\n",
    "    # Background to read current monthly file\n",
    "    # read data dictionary text file \n",
    "    dd_file = dd_matcher[file]\n",
    "    data_dict = open(dd_file, 'r', encoding='iso-8859-1').read()\n",
    "    if dd_file == 'may04dd.txt':\n",
    "        data_dict = data_dict.replace('HRHHID (partII)', 'HRHHID2')\n",
    "\n",
    "    # manually list out the IDs for series of interest \n",
    "    var_names = ['HRMONTH', 'HRYEAR4', 'HRMIS', 'QSTNUM', 'OCCURNUM', \n",
    "                 'HRHHID', 'HRHHID2', 'GESTFIPS', 'HWHHWGT']   \n",
    "\n",
    "    if curr_mo < pd.to_datetime('2004-05-01'):\n",
    "        var_names = ['HRMONTH', 'HRYEAR4', 'HRMIS', 'QSTNUM', 'OCCURNUM', \n",
    "                     'HRHHID', 'HRSAMPLE', 'HRSERSUF', 'HUHHNUM', 'GESTFIPS', \n",
    "                     'HWHHWGT', 'HRYEAR']      \n",
    "\n",
    "    # regular expression matching series name and data dict pattern\n",
    "    p = return_dd_parser(dd_file)\n",
    "\n",
    "    # pick data type based on size of variable\n",
    "    def id_dtype(size, name):\n",
    "        size = int(size)\n",
    "        dtype = ('U4' if name in ['HRSAMPLE']\n",
    "                 else 'U2' if name in ['HRSERSUF']\n",
    "                 else 'intp' if size > 9 \n",
    "                 else 'int32' if size > 4 \n",
    "                 else 'int16' if size > 2 \n",
    "                 else 'int8')\n",
    "        return dtype\n",
    "\n",
    "    # dictionary of variable name: [start, end, and length + 's']\n",
    "    if dd_file in ['jan98dd.asc', 'jan98dd2.asc']:\n",
    "        d = {s[0]: [int(s[2])-1, int(s[2])+int(s[1])-1, \n",
    "                    f'{s[1]}s', id_dtype(s[1], s[0])] \n",
    "             for s in re.findall(p, data_dict) if s[0] in var_names}       \n",
    "    else:\n",
    "        d = {s[0]: [int(s[2])-1, int(s[3]), f'{s[1]}s', id_dtype(s[1], s[0])]\n",
    "         for s in re.findall(p, data_dict) if s[0] in var_names}\n",
    "\n",
    "    # data types\n",
    "    dtypes = [(k, v[-1]) for k, v in d.items()]\n",
    "\n",
    "    # weight variable start and end location\n",
    "    ws, we = d['HWHHWGT'][:2]\n",
    "\n",
    "    # lists of variable starts, ends, and lengths\n",
    "    start, end, width, dtype = zip(*d.values())\n",
    "\n",
    "    # create list of which characters to skip in each row\n",
    "    skip = ([f'{s - e}x' for s, e in zip(start, [0] + list(end[:-1]))])\n",
    "\n",
    "    # create format string by joining skip and variable segments\n",
    "    unpack_fmt = ''.join([j for i in zip(skip, width) for j in i])\n",
    "\n",
    "    # struct can interpret row bytes with the format string\n",
    "    unpacker = struct.Struct(unpack_fmt).unpack_from\n",
    "\n",
    "    # Assign new date variable\n",
    "    date = lambda x: (((x.HRYEAR4.astype(np.int32) * 100) + \n",
    "                      x.HRMONTH.astype(np.int8)) % 10000)\n",
    "    \n",
    "    # 1998 and onward have OCCURNUM to keep first in HH\n",
    "    if curr_mo >= pd.to_datetime('1998-01-01'):\n",
    "\n",
    "    \n",
    "        # Read new monthly file\n",
    "        data = [unpacker(row) for row in open(file, 'rb') \n",
    "                if (row[ws:we].strip() > b'0')]\n",
    "\n",
    "        # Convert to dataframe using specified weights\n",
    "        df = (pd.DataFrame(np.array(data, dtype=dtypes))\n",
    "                .assign(DATE = date))\n",
    "        \n",
    "        # Create HHID2 if necessary\n",
    "        if curr_mo < pd.to_datetime('2004-05-01'):\n",
    "            df['HRHHID2'] = id2_gen(df)\n",
    "            \n",
    "        # Keep only first observation in each HH\n",
    "        df = df.drop_duplicates(subset=['HRHHID', 'HRHHID2'], keep='first')\n",
    "        \n",
    "    else:\n",
    "        # Read new monthly file\n",
    "        data = [unpacker(row) for row in open(file, 'rb') \n",
    "                if (row[ws:we].strip() > b'0')]\n",
    "\n",
    "        # Convert to dataframe using specified weights\n",
    "        df = pd.DataFrame(np.array(data, dtype=dtypes))\n",
    "        \n",
    "        # Create HHID2 if necessary\n",
    "        if curr_mo < pd.to_datetime('2004-05-01'):\n",
    "            df['HRHHID2'] = id2_gen(df)        \n",
    "\n",
    "        # Keep only first observation in each HH\n",
    "        df = df.drop_duplicates(subset=['HRHHID', 'HRHHID2'], keep='first')\n",
    "        \n",
    "        # Create HRYEAR4 from HRYEAR\n",
    "        df['HRYEAR4'] = df['HRYEAR'] + 1900\n",
    "        df = df.drop(['HRYEAR'], axis=1)\n",
    "        \n",
    "        # Assign date\n",
    "        df = df.assign(DATE = date)\n",
    "        \n",
    "        # Create QSTNUM\n",
    "        df['ID'] = df['HRHHID'].astype('str') + df['HRHHID2'].astype('str')\n",
    "        df = df[df['ID'].isin(qstnum_map[curr_mo_short].keys())]\n",
    "        df['QSTNUM'] = df['ID'].map(qstnum_map[curr_mo_short])\n",
    "\n",
    "    # Rename HHIDs\n",
    "    df = df.rename({'HRHHID': 'HHID', 'HRHHID2': 'HHID2'}, axis=1)\n",
    "\n",
    "    # Need to map state to state id codes\n",
    "    df['STATE'] = df['GESTFIPS'].map(StatesMap)\n",
    "\n",
    "    # Drop GESTFIPS and OCCURNUM\n",
    "    df = df.drop(['GESTFIPS'], axis=1)\n",
    "        \n",
    "    tot_hh = len(df)\n",
    "    print('Total HHs in sample:', tot_hh)\n",
    "\n",
    "    # Read potential match data\n",
    "    keep_cols = ['YEAR', 'MONTH', 'MIS', 'HHID', 'HHID2', 'QSTNUM', \n",
    "                 'OCCURNUM', 'STATE']\n",
    "\n",
    "    date = lambda x: (((x.YEAR.astype(np.int32) * 100) + \n",
    "                      x.MONTH.astype(np.int8)) % 10000)\n",
    "    \n",
    "    mdf = (pd.concat(\n",
    "        [(pd.read_feather(f'clean/cps{year}.ft', columns=keep_cols)\n",
    "            .assign(DATE = date))\n",
    "         for year in years], sort=False))\n",
    "    \n",
    "    subset = ['YEAR', 'MONTH', 'HHID', 'HHID2']\n",
    "    \n",
    "    mdf = mdf.drop_duplicates(subset=subset, keep='first')\n",
    "\n",
    "    mdf = (mdf[mdf['DATE'].isin(yymms)].drop(['MONTH', 'YEAR'], axis=1))\n",
    "\n",
    "    # Merge data\n",
    "    d = {}\n",
    "\n",
    "    # MIS = 1 households get current id\n",
    "    dfmis1 = df.loc[df['HRMIS'] == 1, ['QSTNUM', 'DATE']]\n",
    "    dfmis1['ID'] = dfmis1['DATE'] * 100000 + dfmis1['QSTNUM']\n",
    "    mis1id = dfmis1.set_index('QSTNUM')['ID'].to_dict()\n",
    "    d.update(mis1id)\n",
    "    print('New HHs (MIS1):', len(d))\n",
    "\n",
    "    df = df.loc[df['HRMIS'] > 1]\n",
    "    dft = df\n",
    "\n",
    "    # Loop over MIS and potentional matches to find matched id\n",
    "    for mis in [2, 3, 4, 5, 6, 7, 8]:    \n",
    "        for pm in search_list[mis]:\n",
    "            results = (dft.loc[dft['HRMIS'] == mis]\n",
    "                          .merge(mdf[mdf['DATE'] == pm], \n",
    "                                 on=['HHID', 'HHID2', 'STATE']))\n",
    "\n",
    "            results['ID'] = results['DATE_y'] * 100000 + results['QSTNUM_y']\n",
    "\n",
    "            matched_id = results.set_index('QSTNUM_x')['ID'].to_dict()\n",
    "            print(f'Matched HHs (MIS{mis}): ', len(matched_id))\n",
    "            d.update(matched_id)\n",
    "\n",
    "            dft = dft.loc[~dft['QSTNUM'].isin(matched_id.keys())]\n",
    "\n",
    "        if len(search_list[mis]) > 0:\n",
    "            # Households with no match get current id, same has MIS=1\n",
    "            new_hh = dft[dft['HRMIS'] == mis]\n",
    "            new_hh['ID'] = new_hh['DATE'] * 100000 + new_hh['QSTNUM']\n",
    "            new_hh_d = new_hh.set_index('QSTNUM')['ID'].to_dict()\n",
    "            d.update(new_hh_d)\n",
    "            print(f'Replacement HHs (MIS{mis}): ', len(new_hh_d))\n",
    "            if len(new_hh_d) > 2000:\n",
    "                print('\\nWARNING too many replacements, CHECK!\\n')\n",
    "\n",
    "    print('Total IDs created:', len(d))\n",
    "    print('Total IDs not created:', tot_hh - len(d), '\\n\\n')\n",
    "\n",
    "    monthly_id_dict = {curr_mo: d}\n",
    "\n",
    "    # Save results\n",
    "    cps_ids_full.update(monthly_id_dict)\n",
    "\n",
    "\n",
    "# Write to file\n",
    "with open(ids_file, 'wb') as f:\n",
    "    pickle.dump(cps_ids_full, f)\n",
    "    \n",
    "print('Total months of IDs:', len(cps_ids_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-1994 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T20:29:05.806137Z",
     "start_time": "2019-08-07T20:26:35.277583Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary of unique IDS for 1989-93\n",
    "ids_file = 'CPSID_89-93.pkl'\n",
    "if os.path.isfile(ids_file) == False:\n",
    "    \n",
    "    match_mos = {2: [1],\n",
    "                 3: [2, 1],\n",
    "                 4: [3, 2, 1],\n",
    "                 5: [12, 11, 10, 9],\n",
    "                 6: [13, 12, 11, 10, 1],\n",
    "                 7: [14, 13, 12, 11, 2, 1],\n",
    "                 8: [15, 14, 13, 12, 3, 2, 1]}\n",
    "    \n",
    "    date_range = [(dt.year, dt.month) for dt in \n",
    "                  pd.date_range(start='1989-01-01', end='1996-12-01', freq='MS')]\n",
    "\n",
    "    columns = ['MONTH', 'YEAR', 'STATE', 'MIS', 'HHID', 'HHID2', 'QSTNUM', \n",
    "               'OCCURNUM', 'PULINENO', 'HHNUM', 'BASICWGT']\n",
    "\n",
    "    id2 = lambda x: np.where(x['HHID2'] > 0, x['HHID2'] % 100, x['HHNUM'])\n",
    "    \n",
    "    mdf = (pd.concat([(pd.read_feather(f'clean/cps{year}.ft', columns=columns)\n",
    "                        .query('OCCURNUM == 1')) for year in range(1989, 1997)])\n",
    "             .assign(ID2 = id2))\n",
    "\n",
    "    mdf['DATE'] = ((mdf['YEAR'] * 100) + mdf['MONTH']) % 10000 \n",
    "\n",
    "    combined_ids = {}\n",
    "\n",
    "    for year, month in date_range:\n",
    "\n",
    "        ids = {}\n",
    "\n",
    "        mo_id = (year % 100) * 100 + month\n",
    "        curr_mo = pd.to_datetime(f'{year}-{month}-01')\n",
    "\n",
    "        data = (pd.read_feather(f'clean/cps{year}.ft', columns=columns)\n",
    "                  .query('MONTH == @month and OCCURNUM == 1'))\n",
    "        \n",
    "        if 'HHID2' in data.keys():\n",
    "            data['ID2'] = data['HHID2'] % 100\n",
    "        else:\n",
    "            data['ID2'] = data['HHNUM']\n",
    "\n",
    "        data['DATE'] = mo_id\n",
    "\n",
    "        ndf = data.query('MIS == 1')\n",
    "\n",
    "        ndf['ID'] = ndf['DATE'] *100000 + ndf['QSTNUM']\n",
    "\n",
    "        new_id = ndf.set_index('QSTNUM')['ID'].to_dict()\n",
    "\n",
    "        ids.update(new_id)\n",
    "\n",
    "        # Search for old IDS\n",
    "        search_list = {mis: [(date.year, date.month) for date in \n",
    "                             [curr_mo - pd.DateOffset(months=mos) for mos in offsets]]\n",
    "                       for mis, offsets in match_mos.items()}\n",
    "\n",
    "\n",
    "        for mis, slist in search_list.items():\n",
    "\n",
    "            df = data.query('MIS == @mis')\n",
    "\n",
    "            for i, (syear, smonth) in enumerate(slist):\n",
    "\n",
    "                d = mdf.query('YEAR == @syear and MONTH == @smonth and MIS == (@i + 1)')\n",
    "\n",
    "                if len(d[d['HHID2'] > 0]) > 0:\n",
    "                    results = (df.merge(d, on=['HHID', 'HHID2', 'STATE']))\n",
    "                else:\n",
    "                    results = (df.merge(d, on=['HHID', 'ID2', 'STATE']))\n",
    "\n",
    "                results['ID'] = results['DATE_y'] * 100000 + results['QSTNUM_y']\n",
    "\n",
    "                matched_id = results.set_index('QSTNUM_x')['ID'].to_dict()\n",
    "\n",
    "                ids.update(matched_id)\n",
    "\n",
    "                df = df.query('QSTNUM not in @ids.keys()')\n",
    "\n",
    "            df['ID'] = df['DATE'] * 100000 + df['QSTNUM']\n",
    "\n",
    "            new_id = df.set_index('QSTNUM')['ID'].to_dict()\n",
    "\n",
    "            ids.update(new_id)\n",
    "\n",
    "        combined_ids[curr_mo] = ids\n",
    "\n",
    "    # Write to file\n",
    "    with open(ids_file, 'wb') as f:\n",
    "        pickle.dump(combined_ids, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
