{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bd econ CPS extract\n",
    "\n",
    "bd_CPS_reader.ipynb\n",
    "\n",
    "March 9, 2019\n",
    "\n",
    "Contact: Brian Dew, @bd_econ\n",
    "\n",
    "Requires: `cps_basic_dd.pkl` which is generated by bd_CPS_dd.ipynb\n",
    "\n",
    "-----\n",
    "\n",
    "See [readme](https://github.com/bdecon/econ_data/tree/master/bd_CPS) for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T22:41:01.529125Z",
     "start_time": "2019-04-14T22:41:01.305086Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 0.24.2\n",
      "numpy: 1.16.2\n"
     ]
    }
   ],
   "source": [
    "# Import python packages\n",
    "import pandas as pd\n",
    "print('pandas:', pd.__version__)\n",
    "import numpy as np\n",
    "print('numpy:', np.__version__)\n",
    "import struct\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Map codes for country of birth to country/area names\n",
    "from bd_CPS_details import COB1994Map, COB2007Map\n",
    "\n",
    "os.chdir('/home/brian/Documents/CPS/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T22:41:03.017214Z",
     "start_time": "2019-04-14T22:41:01.530782Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "cpsdd = pickle.load(open('cps_basic_dd.pkl', 'rb'))\n",
    "data_path = '/home/brian/Documents/CPS/data/'\n",
    "data_files = os.listdir(data_path)\n",
    "\n",
    "# Consumer Price Index (retrieve using bd_CPS_cpi)\n",
    "cpi = pd.read_csv('clean/cpi.csv', index_col=[0], parse_dates=True)\n",
    "\n",
    "# Federal minimum wage from DOL:\n",
    "min_wage = [('1981-01-01', '1990-04-01', 3.35),\n",
    "            ('1990-04-01', '1991-04-01', 3.80),\n",
    "            ('1991-04-01', '1996-10-01', 4.25),\n",
    "            ('1996-10-01', '1997-09-01', 4.75),\n",
    "            ('1997-09-01', '2007-07-24', 5.15),\n",
    "            ('2007-07-24', '2008-07-24', 5.85),\n",
    "            ('2008-07-24', '2009-07-24', 6.55),\n",
    "            ('2009-07-24', '2020-12-01', 7.25)]\n",
    "\n",
    "min_wage = [(pd.to_datetime(i[0]), pd.to_datetime(i[1]), i[2])\n",
    "            for i in min_wage]\n",
    "\n",
    "# Unique ID list\n",
    "ids_file = 'CPS_unique_ids.pkl'\n",
    "if os.path.isfile(ids_file):\n",
    "    cps_ids_full = pickle.load(open(ids_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T22:41:03.024107Z",
     "start_time": "2019-04-14T22:41:03.018570Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def id2_gen(np_mo):\n",
    "    \"\"\"Create HRHHID2 for pre May 2004 data\"\"\"\n",
    "    hrsample = [x[1:3] for x in np_mo['HRSAMPLE']]\n",
    "    hrsersuf = [x.strip() for x in np_mo['HRSERSUF']]\n",
    "    sersuf_d = {a: str(ord(a.lower()) - 96).zfill(2) for a in set(hrsersuf)\n",
    "            if a in list(string.ascii_letters)}\n",
    "    sersuf_d.update({'-1': '00', '-1.0': '00', '0': '00'})\n",
    "    sersuf = list(map(sersuf_d.get, hrsersuf))\n",
    "    np_mo.loc[np_mo['HUHHNUM'] < 0, 'HUHHNUM'] = 0\n",
    "    huhhnum = np_mo['HUHHNUM'].astype('U1')\n",
    "    \n",
    "    id2 = [''.join(i) for i in zip(hrsample, sersuf, huhhnum)]\n",
    "\n",
    "    return(np.array(id2, dtype='uint32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T22:41:03.033591Z",
     "start_time": "2019-04-14T22:41:03.025978Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def data_file_reader(file, unpacker, dtypes, ws, we):\n",
    "    \n",
    "    # If person weight > 0, unpack the raw file\n",
    "    if file == 'mar04pub.dat':\n",
    "        data = [unpacker(row) for row in open(file, 'rb')\n",
    "                if b'**' not in row and row[ws:we].strip() > b'0']\n",
    "    elif file[3:5] in ['94', '95']:\n",
    "        data = [unpacker(row.replace(b'\\x00\\x00', b'-1')) \n",
    "                for row in open(file, 'rb')\n",
    "                if row[ws:we].strip() > b'0']\n",
    "    else:\n",
    "        data = [unpacker(row) for row in open(file, 'rb') \n",
    "                if row[ws:we].strip() > b'0']\n",
    "    \n",
    "    # Convert to dataframe using specified weights\n",
    "    df = pd.DataFrame(np.array(data, dtype=dtypes))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-14T22:41:01.310Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def clean_all(df, maps, cpi_vals, date):\n",
    "    \n",
    "    # New variables to add\n",
    "    female = lambda x: pd.Categorical(np.where(x['PESEX'] == 2, 1, 0))\n",
    "    state = lambda x: pd.Categorical(x['GESTFIPS'].map(maps['state']))\n",
    "    region = lambda x: pd.Categorical(x['STATE'].map(maps['region']))\n",
    "    educ = lambda x: pd.Categorical(x['PEEDUCA'].map(maps['educ']))\n",
    "    schenr = lambda x: pd.Categorical(\n",
    "        np.where(x['PESCHENR'] == 1, 1, \n",
    "        np.where(x['PESCHENR'] == 2, 0, np.nan)))\n",
    "    married = lambda x: pd.Categorical(\n",
    "        np.where(x['PRMARSTA'].isin([1, 2, 3]), 1, 0))\n",
    "    wbhao = lambda x: pd.Categorical(\n",
    "        np.where(x['PRDTHSP'].isin(maps['hisp']), 'Hispanic', \n",
    "                 x['PRDTRACE'].map(maps['race'])))\n",
    "    veteran = lambda x: pd.Categorical(\n",
    "        np.where(x['PEAFEVER'] == 1, 1, \n",
    "        np.where(x['PEAFEVER'] == 2, 0, np.nan)))\n",
    "    forborn = lambda x: pd.Categorical(\n",
    "        np.where(x['PRCITSHP'].isin([4, 5]), 1, \n",
    "        np.where(x['PRCITSHP'].isin([1, 2, 3]), 0, np.nan)))\n",
    "    indgrp =  lambda x: pd.Categorical(x['PRMJIND1'].map(maps['ind'])) \n",
    "    mjh = lambda x: pd.Categorical(\n",
    "        np.where(x['PRSJMJ']==2, 1, np.where(x['PRSJMJ']==1, 0, np.nan)))\n",
    "    unemptype = lambda x: pd.Categorical(\n",
    "        np.where(x['PRUNTYPE'].isin([1, 2, 3]), 'Job Loser',\n",
    "        np.where(x['PRUNTYPE'] == 4, 'Job Leaver',\n",
    "        np.where(x['PRUNTYPE'] == 5, 'Re-entrant',\n",
    "        np.where(x['PRUNTYPE'] == 6, 'New Entrant', np.nan)))))\n",
    "    ptecon = lambda x: pd.Categorical(\n",
    "        np.where(x['PRWKSTAT'].isin([3, 6]), 1, \n",
    "        np.where(x['PRWKSTAT'].between(2, 10), 0, np.nan)))\n",
    "    workft = lambda x: pd.Categorical(\n",
    "        np.where(x['PRWKSTAT'].isin([2, 8, 9]), 1,\n",
    "        np.where(x['PRWKSTAT'].between(2, 10), 0, np.nan)))\n",
    "    lfs = lambda x: pd.Categorical(\n",
    "        np.where(x['PEMLR'].isin([1, 2]), 'Employed',\n",
    "        np.where(x['PEMLR'].isin([3, 4]), 'Unemployed',\n",
    "        np.where(x['PEMLR'].isin([5, 6, 7]), 'NILF', np.nan))))\n",
    "    cow1 = lambda x: pd.Categorical(\n",
    "        np.where(x['PEIO1COW'] == 1, 'Federal Government',\n",
    "        np.where(x['PEIO1COW'] == 2, 'State Government',\n",
    "        np.where(x['PEIO1COW'] == 3, 'Local Government',\n",
    "        np.where(x['PEIO1COW'].isin([4, 5]), 'Private',\n",
    "        np.where(x['PEIO1COW'] == 6, 'Self-employed Incorporated',\n",
    "        np.where(x['PEIO1COW'] == 7, 'Self-employed Unincorporated',\n",
    "        np.where(x['PEIO1COW'] == 8, 'Without Pay', np.nan))))))))\n",
    "    cow2 = lambda x: pd.Categorical(\n",
    "        np.where(x['PEIO2COW'] == 1, 'Federal Government',\n",
    "        np.where(x['PEIO2COW'] == 2, 'State Government',\n",
    "        np.where(x['PEIO2COW'] == 3, 'Local Government',\n",
    "        np.where(x['PEIO2COW'].isin([4, 5]), 'Private',\n",
    "        np.where(x['PEIO2COW'] == 6, 'Self-employed Incorporated',\n",
    "        np.where(x['PEIO2COW'] == 7, 'Self-employed Unincorporated',\n",
    "        np.where(x['PEIO2COW'] == 8, 'Without Pay', np.nan))))))))\n",
    "    nilfreason = lambda x: pd.Categorical(\n",
    "        np.where((x['PRWNTJOB']==2) & \n",
    "                 ((x['PEMLR']==6) | (x['PENLFACT'].isin([1, 2]))), \n",
    "                 'Disabled/Ill',\n",
    "        np.where((x['PRWNTJOB']==2) & (x['PENLFACT']==4), 'Family',\n",
    "        np.where((x['PRWNTJOB']==2) & ((x['PEMLR']==5) | (x['PENLFACT']==5)), \n",
    "                 'Retired',\n",
    "        np.where((x['PRWNTJOB']==2) & (x['PENLFACT']==3), 'School',\n",
    "        np.where(x['PEDWWNTO']==1, 'Discouraged',\n",
    "        np.where(x['PEMLR'].isin([5, 6, 7]), 'Other', np.nan)))))))\n",
    "    paidhrly = lambda x: pd.Categorical(\n",
    "        np.where(x['PEERNHRY'] == 1, 1,\n",
    "        np.where(x['PEERNHRY'] == 2, 0, np.nan)))\n",
    "    proxy = lambda x: pd.Categorical(\n",
    "        np.where(x['PUSLFPRX'] == 1, 'Self',\n",
    "        np.where(x['PUSLFPRX'] == 2, 'Proxy',\n",
    "        np.where(x['PUSLFPRX'] == 3, 'Both', np.nan))))\n",
    "    \n",
    "    # Wage variables\n",
    "    wkwage = lambda x: np.where(x['PRERNWA'] > 0, x['PRERNWA'] / 100.0, np.nan)\n",
    "    hrwage = lambda x: (\n",
    "        np.where((x['PRERNHLY'] < 0) & (x['PEHRUSL1'] > 0 ) & \n",
    "                 (x['PRERNWA'] > 0), x['PRERNWA'] / x['PEHRUSL1'], \n",
    "        np.where(x['PRERNHLY'] > 0, x['PRERNHLY'], np.nan)) / 100.0  )\n",
    "    rhrwage = lambda x: (np.where(x['HRWAGE'] > 0, \n",
    "                 x['HRWAGE'] * x['REGION'].map(cpi_vals), np.nan))\n",
    "    rwkwage = lambda x: (np.where(x['WKWAGE'] > 0, \n",
    "                 x['WKWAGE'] * x['REGION'].map(cpi_vals), np.nan))\n",
    "    \n",
    "    # Minimum wage\n",
    "    for start, end, wage in min_wage:\n",
    "        if (date >= start) and (date < end):\n",
    "            fed_min_wage = wage\n",
    "    minwage = lambda x: pd.Categorical(\n",
    "        np.where((x['HRWAGE'] > 0) & (x['HRWAGE'] <= fed_min_wage), 1, \n",
    "        np.where(x['HRWAGE'] > fed_min_wage, 0, np.nan)))\n",
    "    \n",
    "    # Old variables to drop\n",
    "    drop_vars = ['PESEX', 'PEAFEVER', 'PESCHENR', 'PRSJMJ',\n",
    "                 'PRUNTYPE', 'PRWKSTAT', 'PRCITSHP', 'PRERNWA', 'PEERNHRY',\n",
    "                 'PUERN2', 'PRMARSTA', 'PEIO1COW', 'PRWNTJOB', 'PEDWWNTO',\n",
    "                 'PEMLR', 'PENLFACT', 'PEEDUCA', 'PENLFRET', 'PRERNHLY',\n",
    "                 'PUSLFPRX', 'PEIO2COW']\n",
    "    \n",
    "    # Correct weights for implied decimals\n",
    "    df[maps['wgt']] = (df[maps['wgt']] / 10000.0).astype('float32')\n",
    "    \n",
    "    # Assign new variables and drop one ones\n",
    "    df = (df.assign(FEMALE = female,\n",
    "                    STATE = state,\n",
    "                    REGION = region,\n",
    "                    EDUC = educ,\n",
    "                    SCHENR = schenr,\n",
    "                    MARRIED = married,\n",
    "                    WBHAO = wbhao,\n",
    "                    VETERAN = veteran,\n",
    "                    FORBORN = forborn,\n",
    "                    INDGRP = indgrp,\n",
    "                    MJH = mjh,\n",
    "                    UNEMPTYPE = unemptype,\n",
    "                    PTECON = ptecon,\n",
    "                    WORKFT = workft,\n",
    "                    PAIDHRLY = paidhrly,\n",
    "                    PROXY = proxy,\n",
    "                    LFS = lfs,\n",
    "                    COW1 = cow1,\n",
    "                    COW2 = cow2,\n",
    "                    NILFREASON = nilfreason,\n",
    "                    WKWAGE = wkwage,\n",
    "                    HRWAGE = hrwage,\n",
    "                    RHRWAGE = rhrwage,\n",
    "                    RWKWAGE = rwkwage,\n",
    "                    MINWAGE = minwage)\n",
    "            .drop(drop_vars, axis=1)\n",
    "            .rename({'PRTAGE': 'AGE', 'HRMONTH': 'MONTH', 'HRHHID': 'HHID',\n",
    "                     'HRMIS': 'MIS', 'GTCBSA': 'CBSA', 'GTCSA': 'CSA',\n",
    "                     'PRUNEDUR': 'UNEMPDUR',\n",
    "                     'PEHRACTT': 'HRSACTT', 'PEHRUSLT': 'HRSUSLT',\n",
    "                     'PEHRUSL1': 'HRSUSL1', 'PEHRUSL2': 'HRSUSL2',\n",
    "                     'PEHRACT1': 'HRSACT1', 'PEHRACT2': 'HRSACT2'}, axis=1))\n",
    "    \n",
    "    # Wage variables to float32\n",
    "    wage_vars = ['WKWAGE', 'RWKWAGE', 'HRWAGE', 'RHRWAGE']\n",
    "    df[wage_vars] = df[wage_vars].astype('float32')\n",
    "    \n",
    "    # Variables to convert to categorical\n",
    "    cat_vars = ['MIS', 'CBSA', 'CSA', 'PEERNLAB', 'PEERNCOV', 'PRFTLF']\n",
    "    for cat_var in cat_vars:\n",
    "        if cat_var in df.keys():\n",
    "            df[cat_var] = df[cat_var].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-14T22:41:01.311Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def clean_special(dfm, maps, date):\n",
    "    \n",
    "    # New variables in selected dates:\n",
    "    wbhaom = lambda x: pd.Categorical(\n",
    "        np.where(x['PRDTHSP'].isin(maps['hisp']), 'Hispanic', \n",
    "                 x['PRDTRACE'].map(maps['racem'])))\n",
    "    cert = lambda x: pd.Categorical(\n",
    "        np.where(x['PECERT1'] == 1, 1, np.where(x['PECERT1']==2, 0, np.nan)))\n",
    "    ctybirth94 = lambda x: pd.Categorical(x['PENATVTY'].map(COB1994Map))\n",
    "    ctybirth07 = lambda x: pd.Categorical(x['PENATVTY'].map(COB2007Map))\n",
    "    county = lambda x: pd.Categorical(\n",
    "        np.where(x['GTCO'] > 0, x['GTCO'] * 100 + x['GESTFIPS'], 0))\n",
    "    \n",
    "    # Revised weights for December 2007\n",
    "    if date == pd.to_datetime('2007-12-01'):\n",
    "        df_rev = pd.read_feather('clean/cps_dec07_rev.ft')\n",
    "        dfm = pd.merge(dfm, df_rev)\n",
    "        dfm['PWSSWGT'] = dfm['NWSSWGT']\n",
    "        dfm['PWCMPWGT'] = dfm['NWCMPWGT']\n",
    "        dfm = dfm.drop(['NWSSWGT', 'NWCMPWGT'], axis=1)\n",
    "\n",
    "    # Country of origin revised in 2007    \n",
    "    if date.year < 2007:\n",
    "        dfm = dfm.assign(CTYBIRTH = ctybirth94)\n",
    "    if date.year >= 2007:\n",
    "        dfm = dfm.assign(CTYBIRTH = ctybirth07) \n",
    "    dfm = dfm.drop(['PENATVTY'], axis=1)   \n",
    "    dfm['CTYBIRTH'] = dfm['CTYBIRTH'].astype('category')\n",
    "\n",
    "    # Four digit year\n",
    "    if date.year < 1998:\n",
    "        dfm['HRYEAR4'] = dfm['HRYEAR'] + 1900\n",
    "        dfm = dfm.drop(['HRYEAR'], axis=1)\n",
    "        \n",
    "        # Person weight is basic weight\n",
    "        dfm['BASICWGT'] = dfm['PWSSWGT'] \n",
    "        \n",
    "    dfm = dfm.rename({'HRYEAR4': 'YEAR'}, axis=1)\n",
    "\n",
    "    # Person weight is composite weight\n",
    "    if date.year >= 1998:\n",
    "        dfm = dfm.rename({'PWCMPWGT': 'BASICWGT'}, axis=1)\n",
    "\n",
    "    # Detailed race allows identifying more than one race\n",
    "    if date.year > 2002:\n",
    "        dfm = dfm.assign(WBHAOM = wbhaom)\n",
    "    dfm = dfm.drop(['PRDTHSP'], axis=1)\n",
    "    \n",
    "    # Renaming industry and occupation codes\n",
    "    if date.year > 2002:\n",
    "        dfm = dfm.rename({'PRMJOCC1': 'OCCM', 'PRMJOCC2': 'OCC2M', \n",
    "                          'PEIO1OCD': 'OCC', 'PEIO2OCD': 'OCC2', \n",
    "                          'PRDTOCC1': 'OCCD', 'PRDTOCC2': 'OCC2D',\n",
    "                          'PRMJIND1': 'INDM', 'PRMJIND2': 'IND2M', \n",
    "                          'PEIO1ICD': 'IND', 'PEIO2ICD': 'IND2', \n",
    "                          'PRDTIND1': 'INDD', 'PRDTIND2': 'IND2D'}, axis=1)\n",
    "    if date.year <= 2002:\n",
    "        dfm = dfm.rename({'PRMJOCC1': 'OCC80M', 'PRMJOCC2': 'OCC280M', \n",
    "                          'PEIO1OCD': 'OCC80', 'PEIO2OCD': 'OCC280', \n",
    "                          'PRDTOCC1': 'OCC80D', 'PRDTOCC2': 'OCC280D',\n",
    "                          'PRMJIND1': 'IND80M', 'PRMJIND2': 'IND280M', \n",
    "                          'PEIO1ICD': 'IND80', 'PEIO2ICD': 'IND280', \n",
    "                          'PRDTIND1': 'IND80D', 'PRDTIND2': 'IND280D'}, axis=1)\n",
    "    \n",
    "    # Number/children in 1999 before data available = -1\n",
    "    if (date.year == 1999) and (date < pd.to_datetime('1999-11-01')):\n",
    "        dfm['PRNMCHLD'] = -1\n",
    "        dfm['PRCHLD'] = -1\n",
    "    if date.year == 1999:\n",
    "        dfm[['PRNMCHLD', 'PRCHLD']] = dfm[['PRNMCHLD', 'PRCHLD']].astype('int8')\n",
    " \n",
    "    # Professional certification questions\n",
    "    if date.year in [2015, 2016]:\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        rev_df = (pd.read_feather(f'clean/cps_cert{year}.ft')\n",
    "                    .query('MONTH == @month'))\n",
    "        dfm = pd.merge(dfm, rev_df, how='outer')\n",
    "\n",
    "    if date.year >= 2015:\n",
    "        dfm = dfm.assign(CERT = cert).drop(['PECERT1'], axis=1)\n",
    "        \n",
    "    # Disability status\n",
    "    if (date >= pd.to_datetime('2008-06-01') ) and (date.year < 2009):\n",
    "        month = date.month\n",
    "        feather_file = 'clean/cps_disability2008.ft'\n",
    "        rev_df = (pd.read_feather(feather_file)\n",
    "                    .query('MONTH == @month')\n",
    "                    .drop(['MONTH'], axis=1))\n",
    "        dfm = pd.merge(dfm, rev_df, on=['QSTNUM', 'OCCURNUM'], how='outer')\n",
    "    if date >= pd.to_datetime('2008-06-01'):\n",
    "        disability = lambda x: pd.Categorical(\n",
    "            np.where(x['PRDISFLG'] == 1, 1, 0))\n",
    "        dfm = dfm.assign(DISABILITY = disability).drop(['PRDISFLG'], axis=1)\n",
    "   \n",
    "    # Matching HRHHID2 in cases where it must be created manually\n",
    "    if maps['id2'] == True:\n",
    "        dfm['HRHHID2'] = id2_gen(dfm)\n",
    "        dfm = dfm.drop(['HRSAMPLE', 'HRSERSUF', 'HUHHNUM'], axis=1)\n",
    "        \n",
    "    dfm = dfm.rename({'HRHHID2': 'HHID2'}, axis=1)\n",
    "    \n",
    "    # Add QSTNUM and OCCURNUM where not available\n",
    "    if date.year < 1998:\n",
    "        dfm['QSTNUM'] = dfm.groupby(['HHID','HHID2']).ngroup().astype('int32')\n",
    "        dfm['OCCURNUM'] = (dfm.groupby('QSTNUM').cumcount() + 1).astype('int8')\n",
    "    \n",
    "    # Unique household IDS\n",
    "    if date >= pd.to_datetime('1995-05-01'): \n",
    "        ids_file = 'CPS_unique_ids.pkl'\n",
    "        if os.path.isfile(ids_file):\n",
    "            dfm['CPSID'] = dfm['QSTNUM'].map(cps_ids_full[date])\n",
    "            \n",
    "    # County code (state and county combined)\n",
    "    if date >= pd.to_datetime('1995-09-01'): \n",
    "        dfm = dfm.assign(COUNTY = county)\n",
    "        dfm = dfm.drop(['GESTFIPS', 'GTCO'], axis=1)\n",
    "    else:\n",
    "        dfm = dfm.drop(['GESTFIPS'], axis=1)\n",
    "\n",
    "\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-14T22:41:01.312Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def revised_annual_data(df, year):\n",
    "    \n",
    "    # Merge in the 2000-revised weights and io recodes here\n",
    "    if 2000 <= year <= 2002:\n",
    "        rev_wgts = pd.read_feather(f'clean/cps_wgt_rev{year}.ft')\n",
    "        df = pd.merge(df, rev_wgts)\n",
    "        df['BASICWGT'] = df['NWCMPWGT']\n",
    "        df['PWORWGT'] = df['NWORWGT']\n",
    "        df['PWSSWGT'] = df['NWSSWGT']\n",
    "        df = df.drop(['NWCMPWGT', 'NWORWGT', 'NWSSWGT'], axis=1)\n",
    "        # IO recodes\n",
    "        rev_io = pd.read_feather(f'clean/cps_io_rev{year}.ft')\n",
    "        df = pd.merge(df, rev_io)       \n",
    "        \n",
    "    # Merge in revised union data\n",
    "    if year in [2001, 2002]:\n",
    "        rev_df = pd.read_feather(f'clean/cps_union_rev{year}.ft')\n",
    "        df = pd.merge(df, rev_df)\n",
    "        df['PEERNLAB'] = df['NEERNLAB']\n",
    "        df['PEERNCOV'] = df['NEERNCOV']\n",
    "        df = df.drop(['NEERNLAB', 'NEERNCOV'], axis=1)\n",
    "        \n",
    "    # Create UNION and UNIONMEM\n",
    "    union = lambda x: pd.Categorical(\n",
    "        np.where((x['PEERNLAB'] == 1) | (x['PEERNCOV'] == 1), 1, \n",
    "        np.where((x['PEERNLAB'] == 2) & (x['PEERNCOV'] == 2), 0, np.nan)))\n",
    "    unionmem = lambda x: pd.Categorical(\n",
    "        np.where(x['PEERNLAB'] == 1, 1, \n",
    "        np.where(x['PEERNLAB'] == 2, 0, np.nan)))\n",
    "    \n",
    "    df = (df.assign(UNION = union, UNIONMEM = unionmem)\n",
    "            .drop(['PEERNLAB', 'PEERNCOV'], axis=1))\n",
    "    \n",
    "    # General mess clean up area\n",
    "    df = df.assign(YEAR = lambda x: pd.Categorical(x['YEAR']))\n",
    "    df['CTYBIRTH'] = df['CTYBIRTH'].astype('category')\n",
    "    cat_vars = ['PRDTRACE', 'PULINENO', 'PRFTLF', 'MONTH', 'CBSA', 'CSA', \n",
    "                'DISABILITY', 'INDGRP', 'IND80', 'OCC80', 'IND80D', 'OCC80D',\n",
    "                'IND80M', 'OCC80M', 'IND', 'OCC', 'IND2', 'OCC2', 'INDD'\n",
    "                'OCCD', 'OCCM', 'INDM', 'IND2M', 'IND2D', 'OCC2D', 'OCC2M',\n",
    "                'OCC280M', 'IND280M', 'OCC280D', 'IND280D', 'OCC280', 'IND280',\n",
    "                'CMSA', 'MSA', 'COUNTY']\n",
    "    for cat_var in cat_vars:\n",
    "        if cat_var in df.keys():\n",
    "            df[cat_var] = df[cat_var].astype('category')\n",
    "    # Clean up metro status\n",
    "    df['METSTA'] = df['METSTA'].fillna(-1).astype('int8')\n",
    "    wgt_vars = ['BASICWGT', 'PWSSWGT']\n",
    "    df[wgt_vars] = df[wgt_vars].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-14T22:41:01.313Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def cps_to_feather(year_list):\n",
    "    data_dictionary = None \n",
    "    for year in year_list:\n",
    "        \n",
    "        # Get the list of raw data files for the given year\n",
    "        file_ending = f'{str(year)[2:]}pub.dat'\n",
    "        raw_files = [file for file in data_files \n",
    "                     if file.endswith(file_ending)]\n",
    "        \n",
    "        # Loop over individual raw monthly files\n",
    "        combined_data = []\n",
    "        for file in raw_files:\n",
    "            # Date of raw monthly file\n",
    "            date = pd.to_datetime(f'{year}-{file[:3]}-01')\n",
    "            # Month's CPI values (by region)\n",
    "            cpi_vals = cpi.loc[date].to_dict()\n",
    "            \n",
    "            # Identify how to read the raw data file\n",
    "            if data_dictionary != cpsdd['matcher'][file]:\n",
    "                data_dictionary = cpsdd['matcher'][file]\n",
    "                dd_info = cpsdd[data_dictionary]\n",
    "                var_info = dd_info['dd']\n",
    "                ws, we = var_info['PWSSWGT'][:2]\n",
    "                dtypes = [(var_name, var_details[-1]) \n",
    "                          for var_name, var_details in var_info.items()]\n",
    "                var_maps = dd_info['map']\n",
    "                unpack_format = dd_info['unpack_fmt']\n",
    "                unpacker = struct.Struct(unpack_format).unpack_from\n",
    "                \n",
    "            # Read raw monthly data and return pandas dataframe\n",
    "            mo_data = data_file_reader(file, unpacker, dtypes, ws, we)\n",
    "            \n",
    "            # Clean up the data\n",
    "            dfm = clean_all(mo_data, var_maps, cpi_vals, date)\n",
    "            clean_mo_data = clean_special(dfm, var_maps, date)\n",
    "            \n",
    "            combined_data.append(clean_mo_data)\n",
    "            \n",
    "        # Combine monthly files into one annual file\n",
    "        df = (pd.concat(combined_data, sort=False)\n",
    "                .reset_index(drop=True)\n",
    "                .assign(MONTH = lambda x: pd.Categorical(x['MONTH'])))\n",
    "        \n",
    "        # Census revised 2000-based weights and union data\n",
    "        df = revised_annual_data(df, year)\n",
    "        \n",
    "        df.to_feather(f'clean/cps{year}.ft')\n",
    "        obs = len(df)\n",
    "        cols = len(df.keys())\n",
    "        size = round(df.memory_usage().sum() / 1024**2, 1)\n",
    "        print(f'{year} Done: ({obs:,} records, {cols} variables, {size}MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-14T22:41:01.315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994 Done: (1,672,934 records, 65 variables, 175.6MB)\n",
      "1995 Done: (1,648,060 records, 67 variables, 199.7MB)\n",
      "1996 Done: (1,461,469 records, 67 variables, 167.4MB)\n",
      "1997 Done: (1,462,817 records, 67 variables, 167.5MB)\n",
      "1998 Done: (1,461,394 records, 65 variables, 161.8MB)\n",
      "1999 Done: (1,465,602 records, 67 variables, 165.0MB)\n",
      "2000 Done: (1,460,724 records, 79 variables, 203.5MB)\n",
      "2001 Done: (1,560,960 records, 79 variables, 217.5MB)\n",
      "2002 Done: (1,703,017 records, 79 variables, 248.6MB)\n",
      "2003 Done: (1,685,264 records, 70 variables, 197.8MB)\n",
      "2004 Done: (1,656,144 records, 72 variables, 216.5MB)\n",
      "2005 Done: (1,644,787 records, 70 variables, 204.0MB)\n"
     ]
    }
   ],
   "source": [
    "cps_to_feather(range(1994, 2020))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
