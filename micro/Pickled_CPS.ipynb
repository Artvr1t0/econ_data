{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickled_CPS\n",
    "\n",
    "Convert the basic monthly CPS datafiles from 1998 to present into annual pickle files.\n",
    "\n",
    "\n",
    "Updated: February 18, 2018 -- @bd_econ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:15:36.406170Z",
     "start_time": "2018-02-20T02:15:34.749609Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.6\n",
      "pandas 0.22.0\n",
      "numpy 1.14.0\n"
     ]
    }
   ],
   "source": [
    "# Preliminaries and libraries\n",
    "import sys # Check which version of python is being used\n",
    "print(f'python {sys.version_info[0]}.{sys.version_info[1]}')\n",
    "import pandas as pd    # Pandas to organize and make calcs\n",
    "print(f'pandas {pd.__version__}')\n",
    "import numpy as np     # Numpy for calculations\n",
    "print(f'numpy {np.__version__}')\n",
    "import os\n",
    "import re\n",
    "import struct\n",
    "from calendar import month_abbr\n",
    "\n",
    "# Location of data\n",
    "os.chdir('E:/08_Other/Archive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionaries\n",
    "\n",
    "BD Note to self: This section needs some series work! The data dictionaries are small and fast to read, so I just used brute force to read them and manually adjust issues between years. The problem with that method is extending it back to 1994 will be harder, and it makes the code practically unreadable. I should go with some type of dictionary instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:20:48.935090Z",
     "start_time": "2018-02-20T02:20:48.731956Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Convert data dictionaries in to list of variables and their locations\n",
    "\n",
    "# Series of interest \n",
    "s = ['PWORWGT', 'PWCMPWGT', 'HRHHID', 'HURESPLI', 'HRLONGLK', 'HRHHID2', \n",
    "     'PRERNWA', 'PRERNHLY', 'PRTAGE', 'PEAGE', 'PTERNWA', 'PTERNHLY',]\n",
    "\n",
    "# These series can be stored as categorical later on\n",
    "s2 = ['HRMONTH', 'PESEX', 'PEMLR', 'PENLFRET', 'PENLFACT', 'PRDISC', 'GESTFIPS',\n",
    "      'HRMIS', 'PRCOW1', 'PRFTLF', 'PREMPNOT', 'PRCIVLF', 'PEJHRSN', 'PEMJOT',\n",
    "      'PEEDUCA', 'PRWKSTAT', 'PRDTOCC1', 'GTMETSTA', 'GEMETSTA', 'PEDWWNTO']   \n",
    "\n",
    "s = s + s2\n",
    "d = {}\n",
    "\n",
    "p = re.compile('\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)')\n",
    "for y in [2013, 2014, 2015, 2017]:\n",
    "    # Read the data dictionary file to get the column names and locations\n",
    "    dd_txt = f'data/January_{y}_Record_Layout.txt'\n",
    "    dd = open(dd_txt, 'r', encoding='iso-8859-1').read()\n",
    "    # Regular expression for info of interest based on pattern p\n",
    "    d[y] = [(i[0], int(i[3]), int(i[1])) for i in p.findall(dd) if i[0] in s]\n",
    "p = re.compile('\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\s+.*?(\\d\\d*).*?(\\d\\d+)')\n",
    "for v in ['augnov05', 'jan07', 'jan09', 'jan10', 'may12']:\n",
    "    dd = open(f'data/{v}dd.txt', 'r', encoding='iso-8859-1').read()\n",
    "    d[v] = [(i[0], int(i[3]), int(i[1])) for i in p.findall(dd) if i[0] in s]\n",
    "    # Manual touch up of data dictionaries\n",
    "    d[v][5] = ('HRHHID2', 71, 5)\n",
    "    d[v][8] = ('PRTAGE', 122, 2)\n",
    "    d[v][21] = ('PRDTOCC1', 476, 2)\n",
    "\n",
    "# May 2004 to July 2005 dictionary\n",
    "dd = open(f'data/may04dd.txt', 'r', encoding='iso-8859-1').read()\n",
    "d['may04'] = [(i[0], int(i[3]), int(i[1])) for i in p.findall(dd) if i[0] in s]\n",
    "d['may04'][7] = ('PRTAGE', 122, 2)   \n",
    "d['may04'][20] = ('PRDTOCC1', 476, 2)\n",
    "    \n",
    "# 2003 to May 2004 dictionary    \n",
    "dd = open(f'data/jan03dd.txt', 'r', encoding='iso-8859-1').read()\n",
    "d['jan03'] = [(i[0], int(i[3]), int(i[1])) for i in p.findall(dd) if i[0] in s]\n",
    "d['jan03'][6] = ('GTMETSTA', 105, 1)\n",
    "d['jan03'][7] = ('PRTAGE', 122, 2) \n",
    "d['jan03'][20] = ('PRDTOCC1', 476, 2)\n",
    "    \n",
    "# 1998 to 2002 dictionary    \n",
    "p = re.compile('D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+')\n",
    "dd = open(f'data/jan98dd.asc', 'r', encoding='iso-8859-1').read()\n",
    "d['jan98'] = [(i[0], int(i[2]), int(i[1])) for i in p.findall(dd) if i[0] in s]\n",
    "d['jan98'][21] = ('PRERNHLY', 520, 4)\n",
    "d['jan98'][22] = ('PRERNWA', 527, 8)\n",
    "d['jan98'][6] = ('GTMETSTA', 105, 1)\n",
    "\n",
    "# Manually note years where data dictionary does not change\n",
    "d[2016] = d[2015]\n",
    "d[2018] = d[2017]\n",
    "d[2010] = d['jan10']\n",
    "d[2011] = d['jan10']\n",
    "d[2009] = d['jan09']\n",
    "d[2008] = d['jan07']\n",
    "d[2007] = d['jan07']\n",
    "d[2006] = d['augnov05']\n",
    "d[2003] = d['jan03']\n",
    "d[2002] = d['jan98']\n",
    "d[2001] = d['jan98']\n",
    "d[2000] = d['jan98']\n",
    "d[1999] = d['jan98']\n",
    "d[1998] = d['jan98']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for manual use\n",
    "Read a data dictionary, convert .cps extensions to .dat, or unzip files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T03:13:01.844066Z",
     "start_time": "2018-02-19T03:13:01.828439Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Tools for manual use, as needed\n",
    "\n",
    "# Read a data dictionary in the notebook\n",
    "#print(open(f'data/jan10dd.txt', 'r', encoding='iso-8859-1').read())\n",
    "\n",
    "# Unzip files\n",
    "#from zipfile import ZipFile\n",
    "#for file in [f for f in os.listdir('data/') if f.endswith('pub.zip')]:\n",
    "#    with ZipFile(f'data/{file}', 'r') as zip_ref:\n",
    "#        zip_ref.extractall('data/')\n",
    "\n",
    "# Convert .cps file extension into .dat (early data files end in .cps)\n",
    "#for file in [f for f in os.listdir('data/') if f.endswith('cps')]:\n",
    "#    os.rename(f'data/{file}', f'data/{file[:-4]}.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:20:56.998404Z",
     "start_time": "2018-02-20T02:20:56.889019Z"
    },
    "code_folding": [
     0,
     3,
     17,
     28,
     35,
     46
    ]
   },
   "outputs": [],
   "source": [
    "# Set of functions for parsing raw data\n",
    "\n",
    "# Use struct to read files faster \n",
    "def struct_constr(fieldspecs):\n",
    "    \"\"\"Specify which characters to retrieve and which to ignore\"\"\"\n",
    "    unpack_len = 0\n",
    "    unpack_fmt = \"\"\n",
    "    for fieldspec in fieldspecs:\n",
    "        start = fieldspec[1] - 1\n",
    "        end = start + fieldspec[2]\n",
    "        if start > unpack_len:\n",
    "            unpack_fmt += str(start - unpack_len) + \"x\"\n",
    "        unpack_fmt += str(end - start) + \"s\"\n",
    "        unpack_len = end\n",
    "    return struct.Struct(unpack_fmt).unpack_from\n",
    "\n",
    "# Convert valid lines to list\n",
    "def fwf_to_list(filelist, unpacker):\n",
    "    rows = []\n",
    "    for file in filelist:\n",
    "        with open(f'data/{file}', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                row = tuple(map(int, unpacker(line.encode())))\n",
    "                if row[-1] > 0:  # Filter out weightless rows\n",
    "                    rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Convert list of lists to pandas df\n",
    "def list_to_df(row_list, fieldspecs, year):\n",
    "    \"\"\"Store list as pandas dataframe\"\"\"\n",
    "    df = (pd.DataFrame(row_list, columns=[v[0] for v in fieldspecs])\n",
    "          .apply(pd.to_numeric, downcast='signed'))\n",
    "    return df\n",
    "\n",
    "# This is source of problem with 2004 and 2012\n",
    "def special_years(year, fs1, fs2, sm, path):\n",
    "    \"\"\"Handle cases where dictionary is split in middle of year.\n",
    "       Takes two sets of field specifications, and the split month\"\"\"\n",
    "    y, m1, m2 = f'{year}'[2:], list(range(1, sm)), list(range(sm, 13))\n",
    "    files = [[f'{month_abbr[m].lower()}{y}pub.dat' for m in mlist] for mlist in [m1, m2]]\n",
    "    row_list = fwf_to_list(files[0], struct_constr(fs1)) # First set of months\n",
    "    df2 = list_to_df(row_list, fs1, year)               # Store as temp df\n",
    "    row_list = fwf_to_list(files[1], struct_constr(fs2)) # Second set of months\n",
    "    df2.append(list_to_df(row_list, fs2, year)).to_pickle(f'{path}cps_{year}.pkl')  \n",
    "\n",
    "# Manages the other functions\n",
    "def monthly_to_annual(year, path):\n",
    "    \"\"\"Read monthly files and store as one annual file\"\"\"\n",
    "    if year not in [2004, 2005, 2012]:\n",
    "        # Fill list with monthly data from each monthly file\n",
    "        filepath = [f for f in os.listdir('data/') if f.endswith(f'{str(year)[-2:]}pub.dat')]\n",
    "        row_list = fwf_to_list(filepath, struct_constr(d[year]))\n",
    "        df = list_to_df(row_list, d[year], year)\n",
    "        df.to_pickle(f'{path}cps_{year}.pkl')\n",
    "        \n",
    "    if year == 2012: special_years(2012, d[2011], d['may12'], 5, path)\n",
    "    if year == 2005: special_years(2005, d['may04'], d['augnov05'], 8, path)\n",
    "    if year == 2004: special_years(2004, d['jan03'], d['may04'], 5, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the function to selected years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:29:13.745099Z",
     "start_time": "2018-02-20T02:21:41.002696Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Loop or manual\n",
    "path = 'C:/Working/econ_data/micro/data/'\n",
    "\n",
    "for year in range(1998, 2019):\n",
    "    monthly_to_annual(year, path)\n",
    "\n",
    "#monthly_to_annual(2000, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:21:29.407940Z",
     "start_time": "2018-02-20T02:21:29.329818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49123748"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(f'{path}cps_2000.pkl')\n",
    "df.memory_usage(index=True).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
