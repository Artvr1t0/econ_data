{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPS_Reader_Feather\n",
    "\n",
    "##### Updated: March 11, 2018 -- @bd_econ\n",
    "\n",
    "This notebook is used to read monthly CPS datafiles and store them into annual feather format pandas dataframes.\n",
    "\n",
    "\n",
    "TO DO:\n",
    "\n",
    "Function should determine variable dataformat (e.g. np.int8) based on length provided by data dictionary.\n",
    "\n",
    "Take advantage of pandas categoricals to store information about what values mean.\n",
    "\n",
    "Clean up code and add annotations.\n",
    "\n",
    "Generate a consistent variable for race and ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:19:03.280993Z",
     "start_time": "2018-03-16T17:19:03.003724Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import packages and identify datafiles\n",
    "import re\n",
    "import os\n",
    "import struct\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from calendar import month_abbr\n",
    "\n",
    "# Location of data\n",
    "os.chdir('E:/08_Other/Archive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:19:11.101774Z",
     "start_time": "2018-03-16T17:19:11.084385Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Information about dictionary start and end dates and how to parse the columns\n",
    "d = {\n",
    " 'January_2017_Record_Layout.txt': {'start': '2017-01-01', 'end': '2018-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'January_2015_Record_Layout.txt': {'start': '2015-01-01', 'end': '2016-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'January_2014_Record_Layout.txt': {'start': '2014-01-01', 'end': '2014-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'January_2013_Record_Layout.txt': {'start': '2013-01-01', 'end': '2013-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'may12dd.txt': {'start': '2012-05-01', 'end': '2012-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'jan10dd.txt': {'start': '2010-01-01', 'end': '2012-04-30', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan09dd.txt': {'start': '2009-01-01', 'end': '2009-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan07dd.txt': {'start': '2007-01-01', 'end': '2008-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'augnov05dd.txt': {'start': '2005-08-01', 'end': '2006-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'may04dd.txt': {'start': '2004-05-01', 'end': '2005-7-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan03dd.txt': {'start': '2003-01-01', 'end': '2004-04-30', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan98dd.asc': {'start': '1998-01-01', 'end': '2002-12-31', 'ixno': 2, 'split_string':'    ', 'p1': 'D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+', 'p2': 'D {var[0]}.*?(V .*?)\\n(?:\\x0c)?D ', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'sep95_dec97_dd.txt': {'start': '1995-09-01', 'end': '1997-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:19:16.828134Z",
     "start_time": "2018-03-16T17:19:16.818626Z"
    },
    "code_folding": [
     0,
     6,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# Series of interest \n",
    "s3 = ['PWORWGT', 'PWCMPWGT', 'HRHHID2', 'PRERNWA', 'PTERNWA', 'PWSSWGT']\n",
    "\n",
    "s2 = ['PEHRUSL1', 'HRYEAR', 'HRYEAR4', 'PRUNEDUR', 'PRERNHLY', 'PTERNHLY']\n",
    "\n",
    "# These series can be stored as categorical later on\n",
    "s1 = ['HRMONTH', 'PESEX', 'PEMLR', 'PENLFRET', 'PENLFACT', 'PRDISC', 'GESTFIPS',\n",
    "      'HRMIS', 'PRCOW1', 'PRFTLF', 'PREMPNOT', 'PRCIVLF', 'PEJHRSN','PRSJMJ', \n",
    "      'PEEDUCA', 'PRWKSTAT', 'PRMJOCC1', 'GTMETSTA', 'GEMETSTA', 'PEDWWNTO',\n",
    "      'PRUNTYPE', 'PRMJIND1', 'PERACE', 'PTDTRACE', 'PRDTRACE', 'PRORIGIN',\n",
    "      'PRDTHSP', 'PRCHLD', 'PRTAGE', 'PEAGE', 'PULINENO']   \n",
    "s = s1 + s2 + s3 + ['HRHHID']\n",
    "\n",
    "def text_repl(string_item):\n",
    "    return (string_item.replace('PEAGE', 'PRTAGE').replace('PTERNHLY', 'PRERNHLY')\n",
    "            .replace('PTERNWA', 'PRERNWA').replace('GEMETSTA', 'GTMETSTA')\n",
    "            .replace('PERACE', 'PTDTRACE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:19:30.294184Z",
     "start_time": "2018-03-16T17:19:29.600711Z"
    },
    "code_folding": [
     0,
     1,
     46
    ]
   },
   "outputs": [],
   "source": [
    "# Build up dictionary with variable locations and codes\n",
    "for ddi in d.items():\n",
    "    dd = open(f'data/{ddi[0]}', 'r', encoding='iso-8859-1').read()\n",
    "    p1 = re.compile(ddi[1]['p1'])\n",
    "    vlist = [(text_repl(i[0]), int(i[ddi[1]['ixno']]), int(i[1])) \n",
    "             for i in p1.findall(dd) if i[0] in s]\n",
    "    gclist = [(i[0], int(i[ddi[1]['ixno']]), int(i[1])) \n",
    "             for i in p1.findall(dd) if i[0] in s1]\n",
    "    d[ddi[0]]['variables'] = {}\n",
    "    d[ddi[0]]['vlist'] = vlist\n",
    "    p3 = re.compile(ddi[1]['p3'])\n",
    "\n",
    "    # Get the list of codes and their values for each coded variable\n",
    "    for var in vlist:\n",
    "        td = {}\n",
    "        td['start'] = var[1]\n",
    "        td['length'] = var[2]\n",
    "        td['values'] = 'can be used directly'\n",
    "        if var[0] in [g[0] for g in gclist]:\n",
    "            p2 = re.compile(f'\\n(?:\\x0c)?{var[0]}{ddi[1][\"p2\"]}', \n",
    "                            re.MULTILINE|re.DOTALL)\n",
    "            if ddi[0][-1] == 'c':\n",
    "                p2 = re.compile(f'D {var[0]}.*?(V .*?)\\n(?:\\x0c)?D ', \n",
    "                                re.MULTILINE|re.DOTALL)\n",
    "            vals = [[x.strip() for x in i.split('\\n') if len(x.strip()) > 0] \n",
    "                    for i in p2.findall(dd)][0]\n",
    "            val_list = [(i[0], i[1]) for i in \n",
    "                        [p3.findall(v)[0] for v in vals \n",
    "                         if len(p3.findall(v)) > 0]]\n",
    "            td['values'] = val_list\n",
    "            d[ddi[0]]['variables'][var[0]] = td\n",
    "\n",
    "    # Special code to capture state codes, which are stored in two columns\n",
    "    state_vals = []\n",
    "    if ddi[0][-1] == 'c':\n",
    "        d[ddi[0]]['variables']['GESTFIPS']['values'] = state_vals\n",
    "    else:\n",
    "        for state in d[ddi[0]]['variables']['GESTFIPS']['values']:\n",
    "            if (' ' in state[1]) or ('\\t' in state[1]):\n",
    "                split_string = ddi[1]['split_string']\n",
    "                stsplit = state[1].split(split_string)\n",
    "                state1 = tuple([state[0], stsplit[0].strip()])\n",
    "                state2 = tuple([int(stsplit[1].strip()[:2]), \n",
    "                                stsplit[1].strip()[-2:]])\n",
    "                state_vals.append(state1)\n",
    "                state_vals.append(state2)\n",
    "            else:\n",
    "                state_vals.append(state)\n",
    "        d[ddi[0]]['variables']['GESTFIPS']['values'] = state_vals\n",
    "        \n",
    "with open('cps_dictionaries.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:19:34.047346Z",
     "start_time": "2018-03-16T17:19:33.978110Z"
    },
    "code_folding": [
     0,
     3,
     16,
     25,
     37
    ]
   },
   "outputs": [],
   "source": [
    "# Set of functions for parsing raw data\n",
    "\n",
    "# Use struct to read files faster \n",
    "def struct_constr(fieldspecs):\n",
    "    \"\"\"Specify which characters to retrieve and which to ignore\"\"\"\n",
    "    unpack_len = 0\n",
    "    unpack_fmt = \"\"\n",
    "    for fieldspec in fieldspecs:\n",
    "        start = fieldspec[1] - 1\n",
    "        end = start + fieldspec[2]\n",
    "        if start > unpack_len:\n",
    "            unpack_fmt += str(start - unpack_len) + \"x\"\n",
    "        unpack_fmt += str(end - start) + \"s\"\n",
    "        unpack_len = end\n",
    "    return struct.Struct(unpack_fmt).unpack_from\n",
    "\n",
    "def fwf_to_list(file, unpacker, fieldspecs):\n",
    "    \"\"\"Return list of substrings\"\"\"\n",
    "    fw = [i for i in fieldspecs if i[0] == 'PWSSWGT'][0]\n",
    "    #Read monthly file and add it to annual dataframe\n",
    "    return [tuple(map(int, unpacker(line))) \n",
    "            for line in open(f'data/{file}', 'rb') \n",
    "            if int(unpacker(line)[-1]) > 0]\n",
    "\n",
    "# Convert list of lists to pandas df\n",
    "def list_to_df(row_list, fieldspecs):\n",
    "    \"\"\"Store list as pandas dataframe\"\"\"\n",
    "    df = (pd.DataFrame(row_list, columns=[v[0] for v in fieldspecs]))\n",
    "    df[[s for s in s1 if s in df]] = df[[s for s in s1 if s in df]].astype(np.int8)    \n",
    "    df[[s for s in s2 if s in df]] = df[[s for s in s2 if s in df]].astype(np.int16)\n",
    "    df[[s for s in s3 if s in df]] = df[[s for s in s3 if s in df]].astype(np.int32)\n",
    "#    id_vars = ['HRHHID', 'HRHHID2', 'PULINENO']\n",
    "#    df['per_id'] = df['GESTFIPS'].astype(str).str.cat(\n",
    "#        others=[df[i].astype(str) for i in id_vars if i in df.keys()])\n",
    "    return df\n",
    "\n",
    "# Manages the other functions\n",
    "def monthly_to_annual(year, d, path):\n",
    "    \"\"\"Read monthly files and store as one annual file\"\"\"\n",
    "    mo_list = [f for f in os.listdir('data/') \n",
    "               if f.endswith(f'{str(year)[2:]}pub.dat')]\n",
    "    dt_list = [pd.to_datetime(f'{year}-{m[:3]}-01') for m in mo_list]\n",
    "    dd_list = [i[0] for i in d.items() for date in dt_list \n",
    "               if date in pd.date_range(i[1]['start'], i[1]['end'])]\n",
    "\n",
    "    files = list(zip(mo_list, dt_list, dd_list))\n",
    "    df = pd.concat([list_to_df(\n",
    "        fwf_to_list(f[0], struct_constr(d[f[2]]['vlist']), d[f[2]]['vlist']), \n",
    "        d[f[2]]['vlist']) for f in files]).reset_index(drop=True)\n",
    "    df.to_feather(f'{path}cps_{year}.ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:21:08.613659Z",
     "start_time": "2018-03-16T17:21:04.243250Z"
    },
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Read data, year by year and feather\n",
    "path = 'C:/Working/econ_data/micro/data/'\n",
    "for year in range(2018, 2019):\n",
    "    monthly_to_annual(year, d, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
