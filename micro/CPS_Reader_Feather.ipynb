{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPS_Reader_Feather\n",
    "\n",
    "##### Updated: April 8, 2018 -- Brian Dew, @bd_econ\n",
    "\n",
    "This notebook is used to read monthly CPS datafiles and store them into annual feather format pandas dataframes.\n",
    "\n",
    "\n",
    "TO DO:\n",
    "\n",
    "Function should determine variable dataformat (e.g. np.int8) based on length provided by data dictionary.\n",
    "\n",
    "Take advantage of pandas categoricals to store information about what values mean.\n",
    "\n",
    "Clean up code and add annotations. The data dictionary reader is barely used, confusing to read, and doesn't work that well.\n",
    "\n",
    "Thoroughly test and benchmark output against BLS published estimates.\n",
    "\n",
    "Adjust weight variables for implied decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T01:47:39.087623Z",
     "start_time": "2018-04-16T01:47:36.929899Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import packages and identify datafiles\n",
    "import re\n",
    "import os\n",
    "import struct\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from calendar import month_abbr\n",
    "\n",
    "# Location of data\n",
    "os.chdir('E:/08_Other/Archive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T01:47:39.110647Z",
     "start_time": "2018-04-16T01:47:39.089627Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Information about dictionary start and end dates and how to parse the columns\n",
    "# Grouped by data dictionaries, which change when the underlying raw data changes\n",
    "# First item is dictionary name. By name we have start and end dates YYYY-MM-DD.\n",
    "# ixno is the location of the start of each variable of interest\n",
    "# split_string is the gap between state codes and two letter state abbreviations\n",
    "# p1 is the main pattern for variable information\n",
    "# p2 works with the variable name to return the list of values\n",
    "# p3 is the pattern for taking p2 into a tuple of (value_key, value_name)\n",
    "\n",
    "d = {\n",
    " 'January_2017_Record_Layout.txt': {'start': '2017-01-01', 'end': '2018-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'January_2015_Record_Layout.txt': {'start': '2015-01-01', 'end': '2016-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'January_2014_Record_Layout.txt': {'start': '2014-01-01', 'end': '2014-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'January_2013_Record_Layout.txt': {'start': '2013-01-01', 'end': '2013-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'may12dd.txt': {'start': '2012-05-01', 'end': '2012-12-31', 'ixno': 3, 'split_string':'\\t\\t', 'p1': '\\n(\\w+)\\s+(\\d+)\\s+(.*?)\\t+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\t+(.*)'},\n",
    " 'jan10dd.txt': {'start': '2010-01-01', 'end': '2012-04-30', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan09dd.txt': {'start': '2009-01-01', 'end': '2009-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan07dd.txt': {'start': '2007-01-01', 'end': '2008-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'augnov05dd.txt': {'start': '2005-08-01', 'end': '2006-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'may04dd.txt': {'start': '2004-05-01', 'end': '2005-7-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan03dd.txt': {'start': '2003-01-01', 'end': '2004-04-30', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan98dd.asc': {'start': '1998-01-01', 'end': '2002-12-31', 'ixno': 2, 'split_string':'    ', 'p1': 'D (\\w+)\\s+(\\d{1,2})\\s+(\\d+)\\s+', 'p2': 'D {var[0]}.*?(V .*?)\\n(?:\\x0c)?D ', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'sep95_dec97_dd.txt': {'start': '1995-09-01', 'end': '1997-12-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jun95_aug95_dd.txt': {'start': '1995-06-01', 'end': '1995-08-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'apr94_may95_dd.txt': {'start': '1994-04-01', 'end': '1995-05-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'},\n",
    " 'jan94_mar94_dd.txt': {'start': '1994-01-01', 'end': '1995-03-31', 'ixno': 3, 'split_string':'    ', 'p1': '\\n(?:\\x0c)?(\\w+)\\s+(\\d+)\\s+(.*?) \\s+.*?(\\d\\d*).*?(\\d\\d+)', 'p2': '.*?VALID ENTRIES\\n\\n\\s+(.*?)\\n(?:\\x0c)?\\w+\\s+\\d+\\s+.*? \\s+.*?\\d\\d*.*?\\d\\d+', 'p3': '(\\d+.*?\\d*)\\s+(.*)'}    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T01:47:39.126161Z",
     "start_time": "2018-04-16T01:47:39.113150Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Series of interest \n",
    "s3 = ['PWORWGT', 'PWCMPWGT', 'HRHHID2', 'PRERNWA', 'PTERNWA', 'PWSSWGT']\n",
    "\n",
    "s2 = ['PEHRUSL1', 'HRYEAR', 'HRYEAR4', 'PRUNEDUR', 'PRERNHLY', 'PTERNHLY']\n",
    "\n",
    "# These series can be stored as categorical later on\n",
    "s1 = ['HRMONTH', 'PESEX', 'PEMLR', 'PENLFRET', 'PENLFACT', 'PRDISC', 'GESTFIPS',\n",
    "      'HRMIS', 'PRCOW1', 'PRFTLF', 'PREMPNOT', 'PRCIVLF', 'PEJHRSN','PRSJMJ', \n",
    "      'PEEDUCA', 'PRWKSTAT', 'PRMJOCC1', 'GTMETSTA', 'GEMETSTA', 'PEDWWNTO',\n",
    "      'PRUNTYPE', 'PRMJIND1', 'PERACE', 'PTDTRACE', 'PRDTRACE', 'PRORIGIN',\n",
    "      'PRDTHSP', 'PRCHLD', 'PRTAGE', 'PEAGE', 'PULINENO', 'PRWNTJOB', 'PEERNLAB']   \n",
    "s = s1 + s2 + s3 + ['HRHHID']\n",
    "\n",
    "def text_repl(string_item):\n",
    "    return (string_item.replace('PEAGE', 'PRTAGE').replace('PTERNHLY', 'PRERNHLY')\n",
    "            .replace('PTERNWA', 'PRERNWA').replace('GEMETSTA', 'GTMETSTA')\n",
    "            .replace('PERACE', 'PRDTRACE').replace('PTDTRACE', 'PRDTRACE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T01:47:42.972131Z",
     "start_time": "2018-04-16T01:47:39.128163Z"
    },
    "code_folding": [
     0,
     46,
     50
    ]
   },
   "outputs": [],
   "source": [
    "# Build up dictionary with variable locations and codes\n",
    "for ddi in d.items():\n",
    "    dd = open(f'data/{ddi[0]}', 'r', encoding='iso-8859-1').read()\n",
    "    p1 = re.compile(ddi[1]['p1'])\n",
    "    vlist = [(text_repl(i[0]), int(i[ddi[1]['ixno']]), int(i[1])) \n",
    "             for i in p1.findall(dd) if i[0] in s]\n",
    "    gclist = [(i[0], int(i[ddi[1]['ixno']]), int(i[1])) \n",
    "             for i in p1.findall(dd) if i[0] in s1]\n",
    "    d[ddi[0]]['variables'] = {}\n",
    "    d[ddi[0]]['vlist'] = vlist\n",
    "    p3 = re.compile(ddi[1]['p3'])\n",
    "\n",
    "    # Get the list of codes and their values for each coded variable\n",
    "    for var in vlist:\n",
    "        td = {}\n",
    "        td['start'] = var[1]\n",
    "        td['length'] = var[2]\n",
    "        td['values'] = 'can be used directly'\n",
    "        if var[0] in [g[0] for g in gclist]:\n",
    "            p2 = re.compile(f'\\n(?:\\x0c)?{var[0]}{ddi[1][\"p2\"]}', \n",
    "                            re.MULTILINE|re.DOTALL)\n",
    "            if ddi[0][-1] == 'c':\n",
    "                p2 = re.compile(f'D {var[0]}.*?(V .*?)\\n(?:\\x0c)?D ', \n",
    "                                re.MULTILINE|re.DOTALL)\n",
    "            vals = [[x.strip() for x in i.split('\\n') if len(x.strip()) > 0] \n",
    "                    for i in p2.findall(dd)][0]\n",
    "            val_list = [(i[0], i[1]) for i in \n",
    "                        [p3.findall(v)[0] for v in vals \n",
    "                         if len(p3.findall(v)) > 0]]\n",
    "            td['values'] = val_list\n",
    "            d[ddi[0]]['variables'][var[0]] = td\n",
    "\n",
    "    # Special code to capture state codes, which are stored in two columns\n",
    "    state_vals = []\n",
    "    if ddi[0][-1] == 'c':\n",
    "        d[ddi[0]]['variables']['GESTFIPS']['values'] = state_vals\n",
    "    else:\n",
    "        for state in d[ddi[0]]['variables']['GESTFIPS']['values']:\n",
    "            if (' ' in state[1]) or ('\\t' in state[1]):\n",
    "                split_string = ddi[1]['split_string']\n",
    "                stsplit = state[1].split(split_string)\n",
    "                state1 = tuple([state[0], stsplit[0].strip()])\n",
    "                state2 = tuple([int(stsplit[1].strip()[:2]), \n",
    "                                stsplit[1].strip()[-2:]])\n",
    "                state_vals.append(state1)\n",
    "                state_vals.append(state2)\n",
    "            else:\n",
    "                state_vals.append(state)\n",
    "        d[ddi[0]]['variables']['GESTFIPS']['values'] = state_vals\n",
    "        \n",
    "with open('cps_dictionaries.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T01:47:42.999255Z",
     "start_time": "2018-04-16T01:47:42.974128Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CEPR Race and Ethnicity variable that is time consistent\n",
    "r_e_d = {'re1': {'race': ('PRDTRACE', [(1, [1]), (2, [2]), (4, [4]), (5, [3, 5])]),\n",
    "                 'hisp': ('PRORIGIN', (3, [1, 2, 3, 4, 5, 6, 7])),\n",
    "                 'start': '1994-01-01',\n",
    "                 'end': '2002-12-01'},\n",
    "         're2': {'race': ('PRDTRACE', [(1, [1]), \n",
    "                                       (2, [2, 6, 10, 11, 12, 15, 16, 19]), \n",
    "                                       (4, [4, 5, 8, 9, 13, 14, 17, 18]), \n",
    "                                       (5, [3, 7, 20, 21])]),\n",
    "                 'hisp': ('PRDTHSP', (3, [1, 2, 3, 4, 5])),\n",
    "                 'start': '2003-01-01',\n",
    "                 'end': '2012-04-01'},\n",
    "         're3': {'race': ('PRDTRACE', [(1, [1]), \n",
    "                                       (2, [2, 6, 10, 11, 12, 16, 17, 18, 22, 23]), \n",
    "                                       (4, [4, 5, 8, 9, 13, 14, 15, 19, 20, 21, 24]), \n",
    "                                       (5, [3, 7, 25, 26])]),\n",
    "                 'hisp': ('PRDTHSP', (3, [1, 2, 3, 4, 5])),\n",
    "                 'start': '2012-05-01',\n",
    "                 'end': '2013-12-01'},\n",
    "         're4': {'race': ('PRDTRACE', [(1, [1]), \n",
    "                                       (2, [2, 6, 10, 11, 12, 16, 17, 18, 22, 23]), \n",
    "                                       (4, [4, 5, 8, 9, 13, 14, 15, 19, 20, 21, 24]), \n",
    "                                       (5, [3, 7, 25, 26])]),\n",
    "                 'hisp': ('PRDTHSP', (3, [1, 2, 3, 4, 5, 6, 7, 8])),\n",
    "                 'start': '2014-01-01',\n",
    "                 'end': '2018-12-01'}}\n",
    "\n",
    "\n",
    "def wbhao_map(row, r_e):\n",
    "    if row[r_e['hisp'][0]] in r_e['hisp'][1][1]:\n",
    "        return r_e['hisp'][1][0]\n",
    "    else:\n",
    "        for r in r_e['race'][1]:\n",
    "            if row[r_e['race'][0]] in r[1]:\n",
    "                return r[0]\n",
    "            \n",
    "def wbhao_mapper(df, year):\n",
    "    if year < 2003:\n",
    "        df['WBHAO'] = df.apply(lambda x: wbhao_map(x, r_e_d['re1']), axis=1)\n",
    "\n",
    "    if 2002 < year < 2012:\n",
    "        df['WBHAO'] = df.apply(lambda x: wbhao_map(x, r_e_d['re2']), axis=1)\n",
    "\n",
    "    if year == 2012:\n",
    "        df.loc[df[df['HRMONTH'] < 5].index, 'WBHAO'] = df.loc[df[df['HRMONTH'] < 5].index, :].apply(\n",
    "            lambda x: wbhao_map(x, r_e_d['re2']), axis=1)\n",
    "        df.loc[df[df['HRMONTH'] >= 5].index, 'WBHAO'] = df.loc[df[df['HRMONTH'] >= 5].index, :].apply(\n",
    "            lambda x: wbhao_map(x, r_e_d['re3']), axis=1)\n",
    "\n",
    "    if year == 2013:\n",
    "        df['WBHAO'] = df.apply(lambda x: wbhao_map(x, r_e_d['re3']), axis=1)\n",
    "\n",
    "    if year > 2013:\n",
    "        df['WBHAO'] = df.apply(lambda x: wbhao_map(x, r_e_d['re4']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T01:47:43.017141Z",
     "start_time": "2018-04-16T01:47:43.000759Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set of functions for parsing raw data\n",
    "\n",
    "# Use struct to read files faster \n",
    "def struct_constr(fieldspecs):\n",
    "    \"\"\"Specify which characters to retrieve and which to ignore\"\"\"\n",
    "    unpack_len = 0\n",
    "    unpack_fmt = \"\"\n",
    "    for fieldspec in fieldspecs:\n",
    "        start = fieldspec[1] - 1\n",
    "        end = start + fieldspec[2]\n",
    "        if start > unpack_len:\n",
    "            unpack_fmt += str(start - unpack_len) + \"x\"\n",
    "        unpack_fmt += str(end - start) + \"s\"\n",
    "        unpack_len = end\n",
    "    return struct.Struct(unpack_fmt).unpack_from\n",
    "\n",
    "def fwf_to_list(file, unpacker, fieldspecs):\n",
    "    \"\"\"Return list of substrings\"\"\"\n",
    "    fw = [i for i in fieldspecs if i[0] == 'PWSSWGT'][0]\n",
    "    #Read monthly file and add it to annual dataframe\n",
    "    return [tuple(map(int, unpacker(line))) \n",
    "            for line in open(f'data/{file}', 'rb') \n",
    "            if int(unpacker(line)[-1]) > 0]\n",
    "\n",
    "# Convert list of lists to pandas df\n",
    "def list_to_df(row_list, fieldspecs):\n",
    "    \"\"\"Store list as pandas dataframe\"\"\"\n",
    "    df = (pd.DataFrame(row_list, columns=[v[0] for v in fieldspecs]))\n",
    "    df[[s for s in s1 if s in df]] = df[[s for s in s1 if s in df]].astype(np.int8)    \n",
    "    df[[s for s in s2 if s in df]] = df[[s for s in s2 if s in df]].astype(np.int16)\n",
    "    df[[s for s in s3 if s in df]] = df[[s for s in s3 if s in df]].astype(np.int32)\n",
    "#    id_vars = ['HRHHID', 'HRHHID2', 'PULINENO']\n",
    "#    df['per_id'] = df['GESTFIPS'].astype(str).str.cat(\n",
    "#        others=[df[i].astype(str) for i in id_vars if i in df.keys()])\n",
    "    return df\n",
    "\n",
    "# Manages the other functions\n",
    "def monthly_to_annual(year, d, path):\n",
    "    \"\"\"Read monthly files and store as one annual file\"\"\"\n",
    "    flist = {f: pd.to_datetime(f'{year}-{f[:3]}-01') \n",
    "             for f in os.listdir('data/') \n",
    "             if f.endswith(f'{str(year)[2:]}pub.dat')}\n",
    "    for dd, vals in d.items():\n",
    "        for f, t in flist.items():\n",
    "            if t in pd.date_range(vals['start'], vals['end']):\n",
    "                flist[f] = dd\n",
    "    df = pd.concat([list_to_df(\n",
    "        fwf_to_list(rd, struct_constr(d[dd]['vlist']), d[dd]['vlist']), \n",
    "        d[dd]['vlist']) for rd, dd in flist.items()]).reset_index(drop=True)\n",
    "    # Additional functions to be applied here, for example cepr_wbhao\n",
    "    df = wbhao_mapper(df, year)\n",
    "    df.to_feather(f'{path}cps_{year}.ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T02:09:36.826546Z",
     "start_time": "2018-04-16T01:47:43.018644Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Read data, year by year and feather\n",
    "path = 'C:/Working/econ_data/micro/data/'\n",
    "for year in range(1994, 2019):\n",
    "    monthly_to_annual(year, d, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
